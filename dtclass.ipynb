{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f457426d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pycaret.regression import *\n",
    "#from multiprocessing import Pool\n",
    "from datetime import datetime\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "import traceback\n",
    "#from os.path import exist\n",
    "\n",
    "pd.set_option('display.min_rows', 21)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a090788f-2ba4-45bf-aa8d-09c6ef22d2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "root=\"/home/sadagopan/amex-default-prediction/\"\n",
    "headers=pd.read_csv(root+\"headers.csv\")\n",
    "headers = headers.columns\n",
    "scols = [col for col in headers if col[0] == 'S']\n",
    "dcols = [col for col in headers if col[0] == 'D']\n",
    "rcols = [col for col in headers if col[0] == 'R']\n",
    "bcols = [col for col in headers if col[0] == 'B']\n",
    "pcols = [col for col in headers if col[0] == 'P']\n",
    "labels =pd.read_csv(root+\"train_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e024910",
   "metadata": {},
   "outputs": [],
   "source": [
    "trains = []\n",
    "for r, d, f in os.walk(root):\n",
    "    f.sort()\n",
    "    for file in f:\n",
    "#        if \"traina\" in file or \"trainb\" in file:\n",
    "        if 'pkl' not in file and 'csv' not in file and (\"xa\" in file or \"xb\" in file or file.startswith(\"xc\")):\n",
    "            trains.append(os.path.join(root, file))\n",
    "\n",
    "dates=pd.DataFrame(pd.date_range('2018-04-01','2019-04-30', freq='MS').tolist(), columns=['S_2_dt'])\n",
    "dates['yearmonth'] = dates['S_2_dt'].dt.strftime(\"%Y%m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2a510522-8c10-426c-86f0-1546aaf99b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#traindf = pd.concat(traindfs, ignore_index=True)\n",
    "headers=pd.read_csv(root+\"headers.csv\")\n",
    "headers = headers.columns\n",
    "\n",
    "traindf = pd.read_csv(root+\"/tdata\")\n",
    "tdf = traindf.groupby('customer_ID')['S_2'].agg(['min', 'max', 'count']).reset_index()\n",
    "#tdf = tdf.merge(labels, on=\"customer_ID\", how=\"inner\")\n",
    "#for i in range(13):\n",
    "#    print(i, tdf[tdf['S_2']==i].count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1d1243a9-4751-474f-8a25-f81257abb215",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf.rename(columns={\"min\": \"mindate\", \"max\": \"maxdate\"}, inplace=True)\n",
    "\n",
    "tdf['maxdate'] = pd.to_datetime(tdf['maxdate'])\n",
    "tdf['mindate'] = pd.to_datetime(tdf['mindate'])\n",
    "\n",
    "#tdf['diffcount'] = tdf.apply(lambda x: ((x['maxdate'] - x['mindate'])/np.timedelta64(1, 'M')))\n",
    "\n",
    "tdf['mondiff'] = (tdf.maxdate.dt.strftime(\"%Y%m\").astype(int) - tdf.mindate.dt.strftime(\"%Y%m\").astype(int) - 87)\n",
    "tdf['mondiff'] = tdf['mondiff'].apply(lambda x: x+88 if x< 0 else x)\n",
    "#tdf[tdf['count'] == tdf['mondiff']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e0b0b6-2114-44bc-b39c-874c9bda058b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf[tdf['customer_ID']=='002b66a636237d16b2344db6c39210bae5f091c9caf8479e6066101b4a197ce0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e21005-cdab-4347-84a7-b6e8c80e7654",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindfs = []\n",
    "\n",
    "for k in range(13):\n",
    "    traindfs.append(pd.DataFrame(columns=headers))\n",
    "k=0\n",
    "for i in range(len(trains)):\n",
    "    print(trains[i])\n",
    "    traindf = pd.read_csv(trains[i], names=headers, header=None)\n",
    "    for j in range(1, 14):\n",
    "        mask = traindf.customer_ID.isin(tdf[tdf['mondiff'] == j].customer_ID)\n",
    "        traindfs[j-1] = pd.concat([traindfs[j-1], traindf[mask]], ignore_index=True)\n",
    "        if traindfs[j-1]['customer_ID'].count() >= 260000:\n",
    "#            traindfs[j-1] = traindfs[j-1].merge(labels, on=\"customer_ID\", how=\"inner\")\n",
    "            traindfs[j-1].iloc[:260000,].to_csv(\"x\" + str(j-1) + \"-\" + str(k) + \".csv\", index=False)\n",
    "            traindfs[j-1] = traindfs[j-1].iloc[260000:,]\n",
    "            print(\"x\" + str(j-1) + \"-\" + str(k) + \".csv\")\n",
    "#            traindfs[j-1].drop(columns=['target'], inplace=True)\n",
    "            k+=1            \n",
    "\n",
    "for i in range(len(traindfs)-1):\n",
    "#    traindfs[i] = traindfs[i].merge(labels, on=\"customer_ID\", how=\"inner\")\n",
    "    traindfs[i].to_csv(\"x\" + str(i) + \".csv\", index=False)\n",
    "    print(\"x\" + str(i) + \".csv\")\n",
    "\n",
    "for i in range(12, len(traindfs)):\n",
    "#    traindfs[i] = traindfs[i].merge(labels, on=\"customer_ID\", how=\"inner\")\n",
    "    traindfs[i].to_csv(\"x\" + str(i) + \"-\" + str(k) + \".csv\", index=False)\n",
    "    print(\"x\" + str(i) + \"-\" + str(k) + \".csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38536ea4-bdf6-47d4-9fe9-37186cf74019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x12-0.csv\n"
     ]
    }
   ],
   "source": [
    "mdf = pd.DataFrame()\n",
    "for m in range(1):\n",
    "    print(\"x12-\" + str(m) + \".csv\")\n",
    "    mdf = pd.concat([mdf, pd.read_csv(\"x12-\"+str(m)+\".csv\")], ignore_index=True)\n",
    "mdf['tag'] = 12\n",
    "mdf['S_2'] = pd.to_datetime(mdf['S_2'])\n",
    "mdf['S_2'] = mdf['S_2'] + pd.offsets.DateOffset(years=1)\n",
    "mdf['S_2'] = mdf['S_2'] + pd.offsets.DateOffset(months=1)\n",
    "mdf['day_of_month'] = mdf['S_2'].dt.strftime(\"%d\")\n",
    "mdf['day_of_week'] = mdf['S_2'].dt.strftime(\"%u\")\n",
    "mdf['day_of_year'] = mdf['S_2'].dt.strftime(\"%j\")\n",
    "mdf['week_of_year'] = mdf['S_2'].dt.isocalendar().week\n",
    "mdf['yearmonth'] = mdf['S_2'].dt.strftime(\"%Y%m\")\n",
    "\n",
    "#mdf = mdf.merge(labels, on=\"customer_ID\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926a11b1-28b3-45d5-bbc8-b4b1a58ee9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindfs = []\n",
    "\n",
    "print(\"Loading data...\", end=\" \")\n",
    "for j in range(12):\n",
    "    print(\"x\" + str(j), end=\" \")\n",
    "    traindfs.append(pd.read_csv(\"x\" + str(j) + \".csv\"))\n",
    "#    traindfs[j]['tag'] = j\n",
    "    traindfs[j]['S_2'] = pd.to_datetime(traindfs[j]['S_2'])\n",
    "    traindfs[j]['day_of_month'] = traindfs[j]['S_2'].dt.strftime(\"%d\")\n",
    "    traindfs[j]['day_of_week'] = traindfs[j]['S_2'].dt.strftime(\"%u\")\n",
    "    traindfs[j]['day_of_year'] = traindfs[j]['S_2'].dt.strftime(\"%j\")\n",
    "    traindfs[j]['week_of_year'] = traindfs[j]['S_2'].dt.isocalendar().week\n",
    "    traindfs[j]['yearmonth'] = traindfs[j]['S_2'].dt.strftime(\"%Y%m\")\n",
    "\n",
    "i=0\n",
    "print(\"\")\n",
    "print(\"Transforming data...\", end=\" \")\n",
    "for j in range(3):\n",
    "    print(\"x\" + str(j), end=\" \")\n",
    "\n",
    "    maxdate = datetime.strptime('Apr 30 2019 1:33PM', '%b %d %Y %I:%M%p')\n",
    "    mindate = maxdate + pd.offsets.DateOffset(months= -1-j)\n",
    "    dates=pd.DataFrame(pd.date_range(mindate, maxdate, freq='MS').tolist(), columns=['S_2_dt'])\n",
    "    dates['yearmonth'] = dates['S_2_dt'].dt.strftime(\"%Y%m\")\n",
    "    dates.reset_index(inplace=True)\n",
    "    dates.rename(columns={'index': 'seq'}, inplace=True)\n",
    "    dates['seq']=(dates['seq']+1)\n",
    "\n",
    "    traindfs[i]['S_2'] = traindfs[i]['S_2'] + pd.offsets.DateOffset(years=1)\n",
    "    traindfs[i]['S_2'] = traindfs[i]['S_2'] + pd.offsets.DateOffset(months=1)\n",
    "    traindfs[i]['yearmonthorig'] = traindfs[i]['S_2'].dt.strftime(\"%Y%m\")\n",
    "    traindfs[i]['seq'] = traindfs[i].groupby(['customer_ID']).cumcount().add(1)\n",
    "    traindfs[i][['sncnt', 'dncnt', 'rncnt', 'bncnt', 'pncnt']] = traindfs[i].apply(lambda x: (sum([(2^scols.index(col)) for col in scols if pd.isnull(x[col])]),\n",
    "                sum([(2^dcols.index(col)) for col in dcols if pd.isnull(x[col])]),\n",
    "                sum([(2^rcols.index(col)) for col in rcols if pd.isnull(x[col])]),\n",
    "                sum([(2^bcols.index(col)) for col in bcols if pd.isnull(x[col])]),\n",
    "                sum([(2^pcols.index(col)) for col in pcols if pd.isnull(x[col])])), axis=1, result_type='expand')\n",
    "\n",
    "    dateshift = pd.DataFrame(traindfs[i].groupby(['customer_ID'])['S_2'].agg(['min', 'max', 'count'])).rename(columns={\"max\": \"S_2\"}).reset_index()\n",
    "    dateshift['S_2'] = pd.to_datetime(dateshift['S_2'])\n",
    "    dateshift['mondiff'] = dateshift['S_2'].dt.month - 4\n",
    "#    dateshift['mondiff'] = dateshift.apply(lambda x: 12+x['mondiff'] if x['S_2'].year == 2018 and x['mondiff'] > j else x['mondiff'], axis=1)\n",
    "    dateshift['mondiff'] = dateshift['mondiff'].apply(lambda x: 12+x if x <= 0 else x)\n",
    "\n",
    "    traindfs[i] = pd.merge(traindfs[i], dateshift[['customer_ID', 'mondiff']], on=['customer_ID'], how='inner')\n",
    "#    traindfs[i]['S_2'] = traindfs[i].apply(lambda x: x['S_2'] - pd.offsets.DateOffset(months=x['mondiff']), axis=1)\n",
    "    traindfs[i]['yearmonth'] = traindfs[i]['S_2'].dt.strftime(\"%Y%m\")\n",
    "\n",
    "    traindfs[i]=pd.merge(traindfs[i], pd.merge(pd.DataFrame(traindfs[i]['customer_ID'].unique(), columns=['customer_ID']), dates, how='cross'), on=['customer_ID', 'yearmonth'], how='outer')\n",
    "#    traindfs[i]['yearmonth'] = traindfs[i].apply(lambda x: x['yearmonth_y'] if pd.isnull(x['yearmonth_x']) else x['yearmonth['S_2_'], axis=1)\n",
    "#    traindfs[i]['origmissrcnt'] = traindfs[i].apply(lambda x: (2**(x['seq_y']-1)) if (pd.isnull(x['S_2'])) else 0, axis=1)\n",
    "#    traindfs[i]['totmissrcnt'] = traindfs[i].apply(lambda x: (2**(x['seq_y']-1)) if (pd.isnull(x['S_2'])) else 0, axis=1)\n",
    "    traindfs[i]['origmissrcnt'] = traindfs[i].apply(lambda x: 1 if (pd.isnull(x['S_2'])) else 0, axis=1)\n",
    "    traindfs[i]['totmissrcnt'] = traindfs[i].apply(lambda x: 1 if (pd.isnull(x['S_2'])) else 0, axis=1)\n",
    "#    dateshift = pd.merge(dateshift, pd.DataFrame(traindfs[i].groupby(['customer_ID'])['origmissrcnt'].sum()).reset_index(), on ='customer_ID', how='inner')\n",
    "#    dateshift = pd.merge(dateshift, pd.DataFrame(traindfs[i].groupby(['customer_ID'])['totmissrcnt'].sum()).reset_index(), on ='customer_ID', how='inner')\n",
    "    traindfs[i]['S_2'] = traindfs[i].apply(lambda x: x['S_2_dt'] if (pd.isnull(x['S_2']) or x['S_2'].month != x['S_2_dt'].month) else x['S_2'], axis=1)\n",
    "#    traindfs[i].drop(columns=['S_2_dt', 'origmissrcnt', 'totmissrcnt', 'seq_x', 'seq_y'], inplace=True)\n",
    "    traindfs[i].drop(columns=['S_2_dt', 'seq_x', 'seq_y'], inplace=True)\n",
    "    traindfs[i] = traindfs[i].rename(columns={\"yearmonth_x\": \"yearmonth\"})\n",
    "\n",
    "    traindfs[i]['yearmonthorig'].fillna(0, inplace=True)\n",
    "    traindfs[i][['sncnt', 'dncnt', 'rncnt', 'bncnt', 'pncnt']].fillna(-1, inplace=True)\n",
    "#    traindfs[i]['mondiff'].fillna(0, inplace=True)\n",
    "    traindfs[i].drop(columns=['mondiff'], inplace=True)\n",
    "\n",
    "#    traindfs[i]['tag'] = j\n",
    "#    traindfs[i] = pd.concat([traindfs[i], pd.merge(mdf, dates['yearmonth'], on=['yearmonth'],how='inner')], ignore_index=True)\n",
    "\n",
    "#    traindfs[i]['day_of_month'] = traindfs[i]['S_2'].dt.strftime(\"%d\")\n",
    "#    traindfs[i]['day_of_week'] = traindfs[i]['S_2'].dt.strftime(\"%u\")\n",
    "#    traindfs[i]['day_of_year'] = traindfs[i]['S_2'].dt.strftime(\"%j\")\n",
    "#    traindfs[i]['week_of_year'] = traindfs[i]['S_2'].dt.isocalendar().week\n",
    "#    traindfs[i]['yearmonth'] = traindfs[i]['S_2'].dt.strftime(\"%Y%m\")\n",
    "    traindfs[i].drop(columns=['S_2'], inplace=True)\n",
    "##    traindfs[i] = traindfs[i].merge(labels, on=\"customer_ID\", how=\"inner\")\n",
    "\n",
    "##    cols=traindfs[i][traindfs[i].select_dtypes(include=['int64', 'float64']).columns].isna().any().index\n",
    "##    cols = [col for col in cols if col[0:1] in ['S','R', 'B', 'D', 'P']]\n",
    "##    traindfs[i][cols] = traindfs[i].groupby(['target', 'day_of_year', 'day_of_week'])[cols].transform(lambda x : x.fillna(x.iloc[np.random.choice(range(0,len(x)))]))\n",
    "##   traindfs[i][cols] = traindfs[i].groupby(['target', 'day_of_year'])[cols].transform(lambda x : x.fillna(x.iloc[np.random.choice(range(0,len(x)))]))\n",
    "\n",
    "##    traindfs[i][cols] = traindfs[i].groupby(['target', 'day_of_month', 'day_of_week'])[cols].transform(lambda x : x.fillna(x.iloc[np.random.choice(range(0,len(x)))]))\n",
    "##    traindfs[i][cols] = traindfs[i].groupby(['target', 'day_of_month'])[cols].transform(lambda x : x.fillna(x.iloc[np.random.choice(range(0,len(x)))]))\n",
    "\n",
    "##    traindfs[i][cols] = traindfs[i].groupby(['target', 'yearmonth', 'day_of_week'])[cols].transform(lambda x : x.fillna(x.iloc[np.random.choice(range(0,len(x)))]))\n",
    "##    traindfs[i][cols] = traindfs[i].groupby(['target', 'yearmonth'])[cols].transform(lambda x : x.fillna(x.iloc[np.random.choice(range(0,len(x)))]))\n",
    "\n",
    "##    traindfs[i][cols] = traindfs[i].groupby(['target', 'day_of_week'])[cols].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "##    traindfs[i][cols] = traindfs[i].groupby(['target'])[cols].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "##    traindfs[i].drop(columns=['target'], inplace=True)\n",
    "##    for k in range(i+1, 12):\n",
    "##        traindfs[i] = pd.concat([traindfs[i], traindfs[k]], ignore_index=True)\n",
    "\n",
    "##    traindfs[i].drop(columns=['S_2'], inplace=True)\n",
    "##    traindfs[i] = traindfs[i].merge(labels, on=\"customer_ID\", how=\"inner\")\n",
    "\n",
    "##    cols=traindfs[i][traindfs[i].select_dtypes(include=['int64', 'float64']).columns].isna().any().index\n",
    "##    cols = [col for col in cols if col[0:1] in ['S','R', 'B', 'D', 'P']]\n",
    "##    traindfs[i][cols] = traindfs[i].groupby(['target', 'day_of_year', 'day_of_week'])[cols].transform(lambda x : x.fillna(x.iloc[np.random.choice(range(0,len(x)))]))\n",
    "##    traindfs[i][cols] = traindfs[i].groupby(['target', 'day_of_year'])[cols].transform(lambda x : x.fillna(x.iloc[np.random.choice(range(0,len(x)))]))\n",
    "\n",
    "##    traindfs[i][cols] = traindfs[i].groupby(['target', 'day_of_month', 'day_of_week'])[cols].transform(lambda x : x.fillna(x.iloc[np.random.choice(range(0,len(x)))]))\n",
    "##    traindfs[i][cols] = traindfs[i].groupby(['target', 'day_of_month'])[cols].transform(lambda x : x.fillna(x.iloc[np.random.choice(range(0,len(x)))]))\n",
    "\n",
    "##    traindfs[i][cols] = traindfs[i].groupby(['target', 'yearmonth', 'day_of_week'])[cols].transform(lambda x : x.fillna(x.iloc[np.random.choice(range(0,len(x)))]))\n",
    "##    traindfs[i][cols] = traindfs[i].groupby(['target', 'yearmonth'])[cols].transform(lambda x : x.fillna(x.iloc[np.random.choice(range(0,len(x)))]))\n",
    "\n",
    "##    traindfs[i][cols] = traindfs[i].groupby(['target', 'day_of_week'])[cols].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "##    traindfs[i][cols] = traindfs[i].groupby(['target'])[cols].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "##    traindfs[i].drop(traindfs[i][traindfs[i]['tag']!=j].index, axis=0, inplace=True)\n",
    "##    traindfs[i].drop(columns=['tag'], inplace=True)\n",
    "\n",
    "    traindfs[i]=traindfs[i].set_index(['customer_ID', 'yearmonth']).unstack(1).sort_index(axis=1, level=1)\n",
    "    traindfs[i] = traindfs[i].reset_index()\n",
    "#    traindfs[i] = pd.merge(traindfs[i], dateshift[['customer_ID', 'origmissrcnt', 'totmissrcnt']], on=['customer_ID'], how='inner')\n",
    "\n",
    "#    traindfs[i].fillna(0, inplace=True)\n",
    "\n",
    "#    cols=traindfs[i].columns\n",
    "#    for col in traindfs[i][traindfs[i].select_dtypes(include=['int64', 'float64']).columns].isna().any().index:\n",
    "#        traindfs[i][col].fillna(traindfs[i][[column for column in cols if col[:len(col)-3] == column[:len(column)-3]]].mean(axis=1), inplace=True)\n",
    "\n",
    "#    for col in traindfs[i][traindfs[i].select_dtypes(include=['int64', 'float64']).columns].isna().any().index:\n",
    "#        traindfs[i][col].fillna(traindfs[i][col].mean(), inplace=True)\n",
    "\n",
    "#    traindfs[i] = traindfs[i].merge(labels, on=\"customer_ID\", how=\"inner\")\n",
    "#    for col in traindfs[i][traindfs[i].select_dtypes(include=['int64', 'float64']).columns].isna().any().index:\n",
    "#        traindfs[i][col].groupby(\"target\")[col].transform(lambda x: x.fillna(x.mean()), inplace=True)\n",
    "#        traindfs[i][col].fillna(-999, inplace=True)\n",
    "\n",
    "#    for col in traindfs[i][traindfs[i].select_dtypes(include=['int64', 'float64']).columns].isna().any().index:\n",
    "#        traindfs[i].groupby(\"target\")[col].transform(lambda x: x.fillna(x.mean()), inplace=True)\n",
    "\n",
    "    traindfs[i] = traindfs[i].merge(labels, on=\"customer_ID\", how=\"inner\")\n",
    "    traindfs[i].drop(columns=[traindfs[i].columns.to_list()[1]], inplace=True)\n",
    "\n",
    "    coldict={}\n",
    "    for col in traindfs[i].columns:\n",
    "        coldict[col] = ''.join(col)\n",
    "    traindfs[i].rename(coldict, axis=1, inplace=True)\n",
    "\n",
    "#    for column in [col for col in traindfs[0].columns if col[len(col)-6:] in ['201904', '201903']]:\n",
    "#        traindfs[0].drop(columns=[column], inplace=True)\n",
    "\n",
    "    traindfs[i].to_csv(\"x\" + str(j) +\"13dts.csv\", index=False)\n",
    "    i+=1\n",
    "\n",
    "traindfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676f386b-eff4-4050-8326-dfab522359c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindfs[0] #[(traindfs[3]['origmissrcnt201904'] >0 ) | (traindfs[3]['origmissrcnt201903']>0) | (traindfs[3]['origmissrcnt201902']>0) | (traindfs[3]['origmissrcnt201901']>0)][['origmissrcnt201901', 'origmissrcnt201902', 'origmissrcnt201903', 'origmissrcnt201904']]\n",
    "#traindfs[0].set_index(['customer_ID', 'yearmonth']).unstack(1).sort_index(axis=1, level=1)\n",
    "traindfs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b7a7ad-bf01-4ab6-a770-e461cec97b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#traindf=pd.read_csv(\"mdfdtcnt.csv\")\n",
    "traindf=pd.DataFrame()\n",
    "for j in range(2):\n",
    "    print(j, end=\" \")\n",
    "    traindf = pd.concat([traindf, traindfs[j]], ignore_index=True)\n",
    "#    cols=traindf[traindf.select_dtypes(include=['int64', 'float64']).columns].isna().any().index\n",
    "#    cols = [col for col in cols if col[0:1] in ['S','R', 'B', 'D', 'P']]\n",
    "#    traindf[cols] = traindf.groupby(['target', 'day_of_year', 'day_of_week'])[cols].transform(lambda x : x.fillna(x.iloc[np.random.choice(range(0,len(x)))]))\n",
    "#    traindf[cols] = traindf.groupby(['target', 'day_of_year'])[cols].transform(lambda x : x.fillna(x.iloc[np.random.choice(range(0,len(x)))]))\n",
    "\n",
    "#    traindf[cols] = traindf.groupby(['target', 'day_of_month', 'day_of_week'])[cols].transform(lambda x : x.fillna(x.iloc[np.random.choice(range(0,len(x)))]))\n",
    "#    traindf[cols] = traindf.groupby(['target', 'day_of_month'])[cols].transform(lambda x : x.fillna(x.iloc[np.random.choice(range(0,len(x)))]))\n",
    "\n",
    "#    traindf[cols] = traindf.groupby(['target', 'yearmonth', 'day_of_week'])[cols].transform(lambda x : x.fillna(x.iloc[np.random.choice(range(0,len(x)))]))\n",
    "#    traindf[cols] = traindf.groupby(['target', 'yearmonth'])[cols].transform(lambda x : x.fillna(x.iloc[np.random.choice(range(0,len(x)))]))\n",
    "\n",
    "#    traindf[cols] = traindf.groupby(['target', 'day_of_week'])[cols].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "#traindf[cols] = traindf.groupby(['target'])[cols].transform(lambda x: x.fillna(x.iloc[np.random.choice(range(0,len(x)))]))\n",
    "\n",
    "cols=traindf[traindf.select_dtypes(include=['int64', 'float64']).columns].isna().any().index\n",
    "cols = [col for col in cols if col[0:1] in ['S','R', 'B', 'D', 'P']]\n",
    "traindf[cols] = traindf.groupby(['target'])[cols].transform(lambda x: x.fillna(x.iloc[np.random.choice(range(0,len(x)))]))\n",
    "traindf[cols] = traindf.groupby(['target'])[cols].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "traindf #[['target', 'B_1201804', 'B_10201804', 'B_11201804', 'B_12201804', 'B_13201804', 'B_14201804', 'B_15201804', 'B_16201804', 'B_1201805', 'B_10201805', 'B_11201805', 'B_12201805', 'B_13201805', 'B_14201805', 'B_15201805', 'B_16201805']]\n",
    "\n",
    "traindf#.groupby(['target'])[cols].transform(lambda x: x.fillna(x.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b757c6c-55fb-4fd2-8af5-97f360dfe385",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf[cols] = traindf.groupby(['target'])[cols].transform(lambda x: x.fillna(x.mean()))\n",
    "cols=traindf[traindf.select_dtypes(include=['int64', 'float64']).columns].isna().any().index\n",
    "cols = [col for col in cols if col[0:1] in ['S','R', 'B', 'D', 'P']]\n",
    "traindf[['target', 'B_1201806', 'B_10201806', 'B_11201806', 'B_12201806', 'B_13201806', 'B_14201806', 'B_15201806', 'B_16201806', 'B_1201805', 'B_10201805', 'B_11201805', 'B_12201805', 'B_13201805', 'B_14201805', 'B_15201805', 'B_16201805']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b1b35b-febc-4253-8a41-57a1338fa3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## grpcols = [col for col in traindfs[i].columns if (col == 'target') or (col[:len(col)-6] in ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']) or ('yearmonth' in col) or ('day_of_week' in col) or ('day_of_month') in col or ('day_of_year' in col)]\n",
    "#grpcols\n",
    "#traindfs[i].groupby(['yearmonth', 'target'])[col].transform(lambda x: x.fillna(x.mean()))\n",
    "#yearmonth \tday_of_month \tday_of_week \tday_of_year \ttarget\n",
    "#traindfs[i].groupby(['target', 'day_of_year', 'day_of_week'])[col].agg(['mean'])\n",
    "#traindfs[i].groupby(['target', 'day_of_month', 'day_of_week'])[col].agg(['mean'])\n",
    "#traindfs[i].groupby(['target', 'yearmonth', 'day_of_week'])[col].agg(['mean'])\n",
    "#traindfs[i].groupby(['target', 'day_of_week'])[col].agg(['mean'])\n",
    "\n",
    "cols=mdf[mdf.select_dtypes(include=['int64', 'float64']).columns].isna().any().index\n",
    "cols = [col for col in cols if col[0:1] in ['S','R', 'B', 'D', 'P']]\n",
    "#mdf[cols] = mdf.groupby(['day_of_year', 'day_of_week'])[cols].transform(lambda x: x.fillna(x.mean()))\n",
    "#mdf[cols] = mdf.groupby(['day_of_year'])[cols].transform(lambda x: x.fillna(x.mean()))\n",
    "#mdf.groupby(['target', 'day_of_year', 'day_of_week']).transform(lambda x: x.fillna(x.mean()))\n",
    "traindfs[1]#[['customer_ID', 'B_39201904', 'B_42201904', 'target']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de7710d-861f-40f1-bb38-af9b67f02554",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindfs = []\n",
    "i=0\n",
    "j=12\n",
    "for k in range(2, 12):\n",
    "    print(\"x\" + str(j) + \"-\" + str(k), end=\" \")\n",
    "    traindfs.append(pd.read_csv(\"x\" + str(j) + \"-\" + str(k) + \".csv\"))\n",
    "\n",
    "    maxdate = datetime.strptime('Apr 30 2019 1:33PM', '%b %d %Y %I:%M%p')\n",
    "    mindate = maxdate + pd.offsets.DateOffset(months= -1-j)\n",
    "    dates=pd.DataFrame(pd.date_range(mindate, maxdate, freq='MS').tolist(), columns=['S_2_dt'])\n",
    "    dates['yearmonth'] = dates['S_2_dt'].dt.strftime(\"%Y%m\")\n",
    "    dates.reset_index(inplace=True)\n",
    "    dates.rename(columns={'index': 'seq'}, inplace=True)\n",
    "    dates['seq']=(dates['seq']+1)\n",
    "\n",
    "    traindfs[i]['S_2'] = pd.to_datetime(traindfs[i]['S_2'])\n",
    "    traindfs[i]['S_2'] = traindfs[i]['S_2'] + pd.offsets.DateOffset(years=1)\n",
    "    traindfs[i]['S_2'] = traindfs[i]['S_2'] + pd.offsets.DateOffset(months=1)\n",
    "    traindfs[i]['yearmonthorig'] = traindfs[i]['S_2'].dt.strftime(\"%Y%m\")\n",
    "    traindfs[i]['seq'] = traindfs[i].groupby(['customer_ID']).cumcount().add(1)\n",
    "    traindfs[i][['sncnt', 'dncnt', 'rncnt', 'bncnt', 'pncnt']] = traindfs[i].apply(lambda x: (sum([(2^scols.index(col)) for col in scols if pd.isnull(x[col])]),\n",
    "                sum([(2^dcols.index(col)) for col in dcols if pd.isnull(x[col])]),\n",
    "                sum([(2^rcols.index(col)) for col in rcols if pd.isnull(x[col])]),\n",
    "                sum([(2^bcols.index(col)) for col in bcols if pd.isnull(x[col])]),\n",
    "                sum([(2^pcols.index(col)) for col in pcols if pd.isnull(x[col])])), axis=1, result_type='expand')\n",
    "\n",
    "    dateshift = pd.DataFrame(traindfs[i].groupby(['customer_ID'])['S_2'].agg(['max', 'count'])).rename(columns={\"max\": \"S_2\"}).reset_index()\n",
    "    dateshift['S_2'] = pd.to_datetime(dateshift['S_2'])\n",
    "    dateshift['mondiff'] = dateshift['S_2'].dt.month - 4\n",
    "#    dateshift['mondiff'] = dateshift.apply(lambda x: 12+x['mondiff'] if x['S_2'].year == 2018 and x['mondiff'] > j else x['mondiff'], axis=1)\n",
    "    dateshift['mondiff'] = dateshift['mondiff'].apply(lambda x: 12+x if x <= 0 else x)\n",
    "\n",
    "    traindfs[i] = pd.merge(traindfs[i], dateshift[['customer_ID', 'mondiff']], on=['customer_ID'], how='inner')\n",
    "#    traindfs[i]['S_2'] = traindfs[i].apply(lambda x: x['S_2'] - pd.offsets.DateOffset(months=x['mondiff']), axis=1)\n",
    "    traindfs[i]['yearmonth'] = traindfs[i]['S_2'].dt.strftime(\"%Y%m\")\n",
    "\n",
    "    traindfs[i]=pd.merge(traindfs[i], pd.merge(pd.DataFrame(traindfs[i]['customer_ID'].unique(), columns=['customer_ID']), dates, how='cross'), on=['customer_ID', 'yearmonth'], how='outer')\n",
    "#    traindfs[i]['yearmonth'] = traindfs[i].apply(lambda x: x['yearmonth_y'] if pd.isnull(x['yearmonth_x']) else x['yearmonth['S_2_'], axis=1)\n",
    "#    traindfs[i]['origmissrcnt'] = traindfs[i].apply(lambda x: (2**(x['seq_y']-1)) if (pd.isnull(x['S_2'])) else 0, axis=1)\n",
    "#    traindfs[i]['totmissrcnt'] = traindfs[i].apply(lambda x: (2**(x['seq_y']-1)) if (pd.isnull(x['S_2'])) else 0, axis=1)\n",
    "    traindfs[i]['origmissrcnt'] = traindfs[i].apply(lambda x: 1 if (pd.isnull(x['S_2'])) else 0, axis=1)\n",
    "    traindfs[i]['totmissrcnt'] = traindfs[i].apply(lambda x: 1 if (pd.isnull(x['S_2'])) else 0, axis=1)\n",
    "#    dateshift = pd.merge(dateshift, pd.DataFrame(traindfs[i].groupby(['customer_ID'])['origmissrcnt'].sum()).reset_index(), on ='customer_ID', how='inner')\n",
    "#    dateshift = pd.merge(dateshift, pd.DataFrame(traindfs[i].groupby(['customer_ID'])['totmissrcnt'].sum()).reset_index(), on ='customer_ID', how='inner')\n",
    "    traindfs[i]['S_2'] = traindfs[i].apply(lambda x: x['S_2_dt'] if (pd.isnull(x['S_2']) or x['S_2'].month != x['S_2_dt'].month) else x['S_2'], axis=1)\n",
    "    traindfs[i].drop(columns=['S_2_dt', 'seq_x', 'seq_y'], inplace=True)\n",
    "#    traindfs[i] = traindfs[i].rename(columns={\"yearmonth_x\": \"yearmonth\"})\n",
    "\n",
    "    traindfs[i]['yearmonthorig'].fillna(0, inplace=True)\n",
    "#    traindfs[i]['mondiff'].fillna(0, inplace=True)\n",
    "    traindfs[i]['day_of_month'] = traindfs[i]['S_2'].dt.strftime(\"%d\")\n",
    "    traindfs[i]['day_of_week'] = traindfs[i]['S_2'].dt.strftime(\"%u\")\n",
    "    traindfs[i]['day_of_year'] = traindfs[i]['S_2'].dt.strftime(\"%j\")\n",
    "    traindfs[i]['week_of_year'] = traindfs[i]['S_2'].dt.isocalendar().week\n",
    "    traindfs[i].drop(columns=['mondiff'], inplace=True)\n",
    "    traindfs[i].drop(columns=['S_2'], inplace=True)\n",
    "\n",
    "#    traindfs[i] = traindfs[i].merge(labels, on=\"customer_ID\", how=\"inner\")\n",
    "\n",
    "#    cols=traindfs[i][traindfs[i].select_dtypes(include=['int64', 'float64']).columns].isna().any().index\n",
    "#    cols = [col for col in cols if col[0:1] in ['S','R', 'B', 'D', 'P']]\n",
    "#    traindfs[i][cols] = traindfs[i].groupby(['target', 'day_of_year', 'day_of_week'])[cols].transform(lambda x : x.fillna(x.iloc[np.random.choice(range(0,len(x)))]))\n",
    "#    traindfs[i][cols] = traindfs[i].groupby(['target', 'day_of_year'])[cols].transform(lambda x : x.fillna(x.iloc[np.random.choice(range(0,len(x)))]))\n",
    "\n",
    "#    traindfs[i][cols] = traindfs[i].groupby(['target', 'day_of_month', 'day_of_week'])[cols].transform(lambda x : x.fillna(x.iloc[np.random.choice(range(0,len(x)))]))\n",
    "#    traindfs[i][cols] = traindfs[i].groupby(['target', 'day_of_month'])[cols].transform(lambda x : x.fillna(x.iloc[np.random.choice(range(0,len(x)))]))\n",
    "\n",
    "#    traindfs[i][cols] = traindfs[i].groupby(['target', 'yearmonth', 'day_of_week'])[cols].transform(lambda x : x.fillna(x.iloc[np.random.choice(range(0,len(x)))]))\n",
    "#    traindfs[i][cols] = traindfs[i].groupby(['target', 'yearmonth'])[cols].transform(lambda x : x.fillna(x.iloc[np.random.choice(range(0,len(x)))]))\n",
    "\n",
    "#    traindfs[i][cols] = traindfs[i].groupby(['target', 'day_of_week'])[cols].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "#    traindfs[i][cols] = traindfs[i].groupby(['target'])[cols].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "#    traindfs[i].drop(columns=['target'], inplace=True)\n",
    "\n",
    "    traindfs[i]=traindfs[i].set_index(['customer_ID', 'yearmonth']).unstack(1).sort_index(axis=1, level=1)\n",
    "    traindfs[i] = traindfs[i].reset_index()\n",
    "#    traindfs[i] = pd.merge(traindfs[i], dateshift[['customer_ID', 'origmissrcnt', 'totmissrcnt']], on=['customer_ID'], how='inner')\n",
    "\n",
    "#    traindfs[i].fillna(0, inplace=True)\n",
    "\n",
    "#    cols=traindfs[i].columns\n",
    "#    for col in traindfs[i][traindfs[i].select_dtypes(include=['int64', 'float64']).columns].isna().any().index:\n",
    "#        traindfs[i][col].fillna(traindfs[i][[column for column in cols if col[:len(col)-3] == column[:len(column)-3]]].mean(axis=1), inplace=True)\n",
    "\n",
    "#    for col in traindfs[i][traindfs[i].select_dtypes(include=['int64', 'float64']).columns].isna().any().index:\n",
    "#        traindfs[i][col].fillna(traindfs[i][col].mean(), inplace=True)\n",
    "\n",
    "#    for col in traindfs[i][traindfs[i].select_dtypes(include=['int64', 'float64']).columns].isna().any().index:\n",
    "#        traindfs[i][col].fillna(-999, inplace=True)\n",
    "\n",
    "    traindfs[i] = traindfs[i].merge(labels, on=\"customer_ID\", how=\"inner\")\n",
    "    traindfs[i].drop(columns=[traindfs[i].columns.to_list()[1]], inplace=True)\n",
    "\n",
    "    coldict={}\n",
    "    for col in traindfs[i].columns:\n",
    "        coldict[col] = ''.join(col)\n",
    "    traindfs[i].rename(coldict, axis=1, inplace=True)\n",
    "\n",
    "#    for column in [col for col in traindfs[0].columns if col[len(col)-6:] in ['201904', '201903']]:\n",
    "#        traindfs[0].drop(columns=[column], inplace=True)\n",
    "\n",
    "    traindfs[i].to_csv(\"x\" + str(j) + \"-\" + str(k) +\"dtcnt.csv\", index=False)\n",
    "    i+=1\n",
    "\n",
    "traindfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72faa854-31d8-4f6e-b973-32cd3495cf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindfs[0].iloc[traindfs[0].notna().index] #[traindfs[i].select_dtypes(include=['int64', 'float64']).columns].isna().any().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef42934",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "traindfs = []\n",
    "\n",
    "for j in range(len(trains)):\n",
    "    i = j#-18\n",
    "    print(trains[j])\n",
    "    traindfs.append(pd.read_csv(trains[j], names=headers, header=None))\n",
    "\n",
    "    traindfs[i]['S_2'] = pd.to_datetime(traindfs[i]['S_2'])\n",
    "    traindfs[i]['S_2'] = traindfs[i]['S_2'] + pd.offsets.DateOffset(years=1)\n",
    "    traindfs[i]['S_2'] = traindfs[i]['S_2'] + pd.offsets.DateOffset(months=1)\n",
    "    traindfs[i]['yearmonth'] = traindfs[i]['S_2'].dt.strftime(\"%Y%m\")\n",
    "\n",
    "    dateshift = pd.DataFrame(traindfs[i].groupby(['customer_ID'])['S_2'].agg(['min', 'count'])).rename(columns={\"min\": \"S_2\"}).reset_index()\n",
    "    dateshift['S_2'] = pd.to_datetime(dateshift['S_2'])\n",
    "    dateshift['mondiff'] = dateshift['S_2'].dt.month - 4\n",
    "    dateshift['mondiff'] = dateshift['mondiff'].apply(lambda x: 12+x if x <0 else x)\n",
    "\n",
    "    traindfs[i] = pd.merge(traindfs[i], dateshift[['customer_ID', 'mondiff']], on=['customer_ID'], how='inner')\n",
    "    traindfs[i]['S_2'] = traindfs[i].apply(lambda x: x['S_2'] - pd.offsets.DateOffset(months=x['mondiff']), axis=1)\n",
    "\n",
    "    traindfs[i]=pd.merge(traindfs[i], pd.merge(pd.DataFrame(traindfs[i]['customer_ID'].unique(), columns=['customer_ID']), dates, how='cross'), on=['customer_ID', 'yearmonth'], how='outer')\n",
    "    traindfs[i]['S_2'] = traindfs[i].apply(lambda x: x['S_2_dt'] if pd.isnull(x['S_2']) else x['S_2'], axis=1)\n",
    "    traindfs[i].drop(columns=['S_2_dt'], inplace=True)\n",
    "\n",
    "    traindfs[i]['mondiff'].fillna(0, inplace=True)\n",
    "#    for col in traindfs[i][traindfs[i].select_dtypes(include=['int64', 'float64']).columns].isna().any().index:\n",
    "#        traindfs[i][col] = traindfs[i].groupby('customer_ID')[col].transform(lambda grp: grp.fillna(np.mean(grp)))\n",
    "#    for col in traindfs[i].columns[traindfs[i].isna().any()]:\n",
    "#        if traindfs[i][col].dtype in ['float64', 'int64']:\n",
    "#            traindfs[i][col].fillna(traindfs[i][col].mean(), inplace=True)\n",
    "    traindfs[i]['day_of_month'] = traindfs[i]['S_2'].dt.strftime(\"%d\")\n",
    "    traindfs[i]['day_of_week'] = traindfs[i]['S_2'].dt.strftime(\"%u\")\n",
    "    traindfs[i]['day_of_year'] = traindfs[i]['S_2'].dt.strftime(\"%j\")\n",
    "    traindfs[i].drop(columns=['mondiff'], inplace=True)\n",
    "    traindfs[i].drop(columns=['S_2'], inplace=True)\n",
    "\n",
    "    traindfs[i]=traindfs[i].set_index(['customer_ID', 'yearmonth']).unstack(1).sort_index(axis=1, level=1)\n",
    "    traindfs[i] = traindfs[i].reset_index()\n",
    "    traindfs[i] = pd.merge(traindfs[i], dateshift[['customer_ID', 'count']], on=['customer_ID'], how='inner')\n",
    "\n",
    "#    for col in traindfs[i][traindfs[i].select_dtypes(include=['int64', 'float64']).columns].isna().any().index:\n",
    "#        traindfs[i][col].fillna(traindfs[i][[column for column in traindfs[i].columns.tolist() if col[:len(col)-3] == column[:len(column)-3]]].mean(axis=1), inplace=True)\n",
    "\n",
    "#    for col in traindfs[i][traindfs[i].select_dtypes(include=['int64', 'float64']).columns].isna().any().index:\n",
    "#        traindfs[i][col].fillna(traindfs[i][col].mean(), inplace=True)\n",
    "\n",
    "    traindfs[i] = traindfs[i].merge(labels, on=\"customer_ID\", how=\"inner\")\n",
    "    traindfs[i].drop(columns=[traindfs[i].columns.to_list()[1]], inplace=True)\n",
    "\n",
    "    coldict={}\n",
    "    for col in traindfs[i].columns:\n",
    "        coldict[col] = ''.join(col)\n",
    "    traindfs[i].rename(coldict, axis=1, inplace=True)\n",
    "\n",
    "    traindfs[i].to_csv(trains[i]+\"nona.csv\", index=False, header=False)\n",
    "\n",
    "traindfs[0][traindfs[0]['customer_ID']==0].to_csv(root+\"/trains/headers.csv\", index=False)\n",
    "traindfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34edcfb-4b4e-4f66-81e1-60eea9fd3084",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#traindf = pd.concat(traindfs, ignore_index=True)\n",
    "headers=pd.read_csv(root+\"headers.csv\")\n",
    "headers = headers.columns\n",
    "\n",
    "traindf = pd.read_csv(root+\"/tdata\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa3f62a-6ddf-438a-b79e-7d3a82e9a1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#traindf = traindf[traindf['target']==1]\n",
    "#for col in [col for col in traindf.columns if (col[len(col)-6:]=='201804')]:\n",
    "#    traindf = traindf[~traindf[col].isnull()]\n",
    "\n",
    "#traindf=traindf[~traindf['B_12201804'].isnull()]\n",
    "#traindf['B_42201804'].unique()\n",
    "\n",
    "#(traindf.groupby('customer_ID')['S_2'].agg([\"min\", \"max\"]).reset_index())['min'].max(), (traindf.groupby('customer_ID')['S_2'].agg([\"min\", \"max\"]).reset_index())['max'].min()\n",
    "\n",
    "tdf = traindf.groupby('customer_ID')['S_2'].count().reset_index()\n",
    "tdf = tdf.merge(labels, on=\"customer_ID\", how=\"inner\")\n",
    "for i in range(1, 13):\n",
    "    print(i, tdf[tdf['S_2']==i].count())\n",
    "\n",
    "#traindf.groupby('customer_ID')['S_2'].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659ec1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers=pd.read_csv(root+\"/trains/headers.csv\")\n",
    "headers = headers.columns\n",
    "\n",
    "traindf=pd.DataFrame()\n",
    "\n",
    "for i in range(12):\n",
    "    traindf = pd.concat([traindf, pd.read_csv(trains[i]+\"new.csv\", names=headers, header=None)], ignore_index=True)\n",
    "print(\"loaded data...\")\n",
    "\n",
    "for col in [col for col in traindf.columns if (col[:len(col)-6] in ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']) or ('yearmonth' in col) or ('day_of_week' in col)]:\n",
    "    traindf[col] = traindf[col].astype(str)\n",
    "    traindf[col] = traindf[col].astype('category')\n",
    "print(\"updated categories...\")\n",
    "\n",
    "traindf.groupby(['target'])['target'].count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a094cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = setup(data=traindf, target='target',\n",
    "          transformation=True,\n",
    "          ignore_features=['customer_ID'],\n",
    "          categorical_features = [col for col in traindf.columns if (col[:len(col)-6] in ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']) or ('yearmonth' in col) or ('day_of_week' in col)],\n",
    "          numeric_features=[col for col in traindf.columns if ('day_of_month') in col or ('day_of_year' in col)],\n",
    "#          normalize = True,\n",
    "          remove_outliers=True, handle_unknown_categorical=True,fix_imbalance=True,\n",
    "          combine_rare_levels=True, create_clusters=True,\n",
    "          feature_selection=True, remove_multicollinearity=True, pca=True, ignore_low_variance=True,\n",
    "          train_size=0.8, fold=6,\n",
    "          use_gpu=True,\n",
    "          silent = True, session_id = 123, verbose=True)\n",
    "save_config(\"/amex/dt3regconfig.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cf08b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#eda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48224110",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load_config(\"/amex/dtclassconfig.pkl\")\n",
    "dt = create_model('dt', fold=6, verbose=True)\n",
    "print(\"created\")\n",
    "save_model(dt, \"/amex/models/dt2reg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e096133-e021-4221-acc5-1ed6b5d7b65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(dt, \"/amex/models/dt1Lclass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e522747b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = tune_model(dt, fold=6, n_iter=10, choose_better=True)\n",
    "print(\"tuned\")\n",
    "save_model(dt, \"e:/rebid/amex/models/dt1Lclasstuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0440bf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = finalize_model(dt)\n",
    "print(\"finalized\")\n",
    "#optimize_threshold(dt)\n",
    "#print(\"optimized\")\n",
    "save_model(dt, \"e:/rebid/amex/models/dt1Lclassfin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3124d641",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrated_dt = calibrate_model(dt)\n",
    "save_model(calibrated_dt, \"/amex/models/dt1Lclasscalib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ade2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(dt, plot='calibration')\n",
    "plot_model(model, plot='calibration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea60f6b-4505-4ccf-8c90-2e9dfd02477f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dtmodels=[]\n",
    "dtsets=[]\n",
    "headers=pd.read_csv(root+\"/trains/headers.csv\")\n",
    "headers = headers.columns\n",
    "n=6\n",
    "algos=['dt', 'catboost', 'lightgbm']\n",
    "loadconfig=True\n",
    "loadmodels=[False, False, False]\n",
    "\n",
    "for i in range(1):\n",
    "    if loadconfig:\n",
    "        dtsets.append(load_config(root+\"/configs/\" + 'dt' + str(n) + \"-\" + str(i) + \"config.pkl\"))\n",
    "    else:\n",
    "        print(i, end=\"\")\n",
    "        traindf=pd.DataFrame()\n",
    "        for j in range(i*n, (i*n)+n):\n",
    "            print(\".\"+str(j), end=\"\")\n",
    "            traindf = pd.concat([traindf, pd.read_csv(trains[i]+\"new1.csv\", names=headers, header=None)], ignore_index=True)\n",
    "\n",
    "        for col in [col for col in traindf.columns if (col[:len(col)-6] in ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']) or ('yearmonth' in col) or ('day_of_week' in col)]:\n",
    "            traindf[col] = traindf[col].astype(str)\n",
    "            traindf[col] = traindf[col].astype('category')\n",
    "\n",
    "        dtsets.append(setup(data=traindf, target='target',\n",
    "              transformation=True,\n",
    "              ignore_features=['customer_ID'],\n",
    "              categorical_features = [col for col in traindf.columns if (col[:len(col)-6] in ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']) or ('yearmonth' in col) or ('day_of_week' in col)],\n",
    "              numeric_features=[col for col in traindf.columns if ('day_of_month') in col or ('day_of_year' in col)],\n",
    "              remove_outliers=True, handle_unknown_categorical=True,\n",
    "              combine_rare_levels=True, \n",
    "              create_clusters=True,\n",
    "#              transform_target=True, transform_target_method='yeo-johnson',\n",
    "              polynomial_features=True,\n",
    "              feature_selection=True, remove_multicollinearity=True, pca=True, ignore_low_variance=True,\n",
    "              train_size=0.8, fold=6,\n",
    "              use_gpu=True,\n",
    "              silent = True, session_id = 123, verbose=True))\n",
    "        save_config(root+\"/configs/\" + algo + str(n) + \"-\" + str(i) + \"config.pkl\")\n",
    "\n",
    "    for j in range(len(algos)):\n",
    "        if loadmodels[j]:\n",
    "            dtmodels.append(load_model(root+\"/models/\" + algos[j] + str(n)+\"-\"+str(i)+\"reg\"))\n",
    "        else:\n",
    "            dtmodels.append(create_model(algos[j], fold=6, verbose=True))\n",
    "            save_model(dtmodels[i], root+\"/models/\" + algos[j] + str(n)+\"-\"+str(i)+\"reg\")\n",
    "\n",
    "dtmodels.append(stack_models(dtmodels, meta_model = dtmodels[len(dtmodels)-1]))\n",
    "save_model(dtmodels[len(dtmodels)-1], root+\"/models/\" + \"\".join(algos) + \"stackedreg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00be89ef-0eca-4ded-9bad-624c3aa46b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 to 12 dates modeling\n",
    "algos=['lightgbm']\n",
    "loadconfig=True\n",
    "loadmodels=[False, False, False, False]\n",
    "d=4\n",
    "nof=4\n",
    "tag=\"new\"\n",
    "xfiles = [\"x\" + str((i*nof)+j) + \"13dts.csv\" for i in range(int(12/nof)) for j in range(nof)]\n",
    "\n",
    "mdf=pd.DataFrame()\n",
    "for m in range(1):\n",
    "    print(\"x12-\"+str(m)+\"dtcnt.csv\", end=\" \")\n",
    "    mdf = pd.concat([mdf, pd.read_csv(\"x12-\"+str(m)+\"dtcnt.csv\")], ignore_index=True)\n",
    "\n",
    "for i in range(1):\n",
    "#    xfiles = [\"x\" + str((i*nof)+j) + \"13dts.csv\" for j in range(nof)]\n",
    "    xfiles = [\"x\" + str(i+j) + \"13dts.csv\" for j in range(12-i)]\n",
    "#    xfiles = [\"x\" + str(i) + \"13dts.csv\"]\n",
    "    dtmodels=[]\n",
    "    tag = \"new\" + str(i)\n",
    "    if loadconfig:\n",
    "        load_config(root+\"/configs/\" + \"x13dts\" + tag + \"config.pkl\")\n",
    "    else:\n",
    "        traindf=pd.DataFrame()\n",
    "        for xfile in xfiles:\n",
    "            print(xfile, end=\" \")\n",
    "            if xfiles.index(xfile) == 0:\n",
    "                traindf = pd.concat([traindf, pd.read_csv(xfile)], ignore_index=True)\n",
    "            else:\n",
    "#                traindf = pd.concat([traindf, pd.read_csv(xfile)[traindf.columns]], ignore_index=True)\n",
    "                traindf = pd.concat([traindf, pd.read_csv(xfile)], ignore_index=True)\n",
    "#            print(\"train count:\", traindf.shape, traindf.groupby(['target'])['customer_ID'].count())\n",
    "\n",
    "#        if i > 4:\n",
    "        print(\"orig train count:\", traindf.shape, traindf.groupby(['target'])['customer_ID'].count())\n",
    "        traindf = pd.concat([traindf, mdf.loc[0:(i*5000),:][traindf.columns]], ignore_index=True)\n",
    "        print(\"train count:\", traindf.shape, traindf.groupby(['target'])['customer_ID'].count())\n",
    "\n",
    "        for col in [col for col in traindf.columns if (col[:len(col)-6] in ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']) or\n",
    "                    ('yearmonth' in col) or ('day_of_week' in col) or ('day_of_month') in col or ('day_of_year' in col)]:\n",
    "            traindf[col] = traindf[col].astype(str)\n",
    "            traindf[col] = traindf[col].astype('category')\n",
    "\n",
    "        s = setup(data=traindf, target='target',\n",
    "            transformation=True,\n",
    "            normalize=True,\n",
    "            ignore_features=['customer_ID'],\n",
    "            categorical_features = [col for col in traindf.columns if (col[:len(col)-6] in ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']) or \n",
    "                                    ('yearmonth' in col) or ('day_of_week' in col) or ('day_of_month') in col or ('day_of_year' in col)],\n",
    "#            numeric_features=[col for col in traindf.columns if ('day_of_month') in col or ('day_of_year' in col)],\n",
    "            remove_outliers=True,\n",
    "            handle_unknown_categorical=True,\n",
    "            combine_rare_levels=True,\n",
    "#            create_clusters=True,\n",
    "#            transform_target=True, transform_target_method='yeo-johnson',\n",
    "            trigonometry_features=True,\n",
    "#            polynomial_features=True, polynomial_degree=2, #d,\n",
    "            feature_selection=True, feature_selection_threshold=0.9,\n",
    "#            remove_multicollinearity=True, #pca=True,\n",
    "            ignore_low_variance=True,\n",
    "            train_size=0.8, fold=10,\n",
    "            use_gpu=True,\n",
    "            silent = True, session_id = 123, verbose=True)\n",
    "        d -= 1\n",
    "        save_config(root+\"/configs/\" + \"x13dts\" + tag + \"config.pkl\")\n",
    "\n",
    "    for j in range(len(algos)):\n",
    "        if loadmodels[j]:\n",
    "            dtmodels.append(load_model(root+\"/models/\" + algos[j] + \"-x13dts\" + tag))\n",
    "        else:\n",
    "            try:\n",
    "                dtmodels.append(create_model(algos[j], fold=10, verbose=True))\n",
    "                save_model(dtmodels[j], root+\"/models/\" + algos[j] + \"-x13dts\" + tag)\n",
    "\n",
    "                if algos[j] in ['dt', 'lightgbm']:\n",
    "                    try:\n",
    "                        dtmodels[j] = tune_model(dtmodels[j], fold=10, search_library = \"scikit-optimize\", n_iter=15, verbose=True)\n",
    "                        save_model(dtmodels[j], root+\"/models/\" + algos[j] + \"-x13dts\" + tag)\n",
    "                    except:\n",
    "                        print(\"Unable to tune for: \", algos[j])\n",
    "                        traceback.print_exc()\n",
    "            except:\n",
    "                print(\"Unable to build model for:\", algos[j])\n",
    "                traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2442700-536d-4760-8b1d-4a53bb0e4371",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf=pd.read_csv(\"mdfdtcnt.csv\")\n",
    "pd.concat([mdf, traindf], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66885cc-ee07-4497-8929-c395bcac96a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtmodels[j] = tune_model(dtmodels[j], fold=10, search_library = \"scikit-optimize\", n_iter=15, verbose=True)\n",
    "save_model(dtmodels[j], root+\"/models/\" + algos[j] + \"-x13dts\" + tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6f10de-0c23-4851-ae37-29423c7bd86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "algos=['dt', 'catboost', 'lightgbm']\n",
    "algos=['lightgbm']\n",
    "loadconfig=True\n",
    "loadmodels=[False, False, False, False]\n",
    "d=6\n",
    "\n",
    "for i in range(12):\n",
    "    dtmodels=[]\n",
    "    if loadconfig:\n",
    "        load_config(root+\"/configs/\" + \"x\" + str(i) + \"config.pkl\")\n",
    "    else    :\n",
    "        print(\"x\" + str(i) + \"13dts.csv\", end=\"\")\n",
    "        traindf = pd.read_csv(\"x\" + str(i) + \"13dts.csv\")\n",
    "\n",
    "        for col in [col for col in traindf.columns if (col[:len(col)-6] in ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']) or\n",
    "                    ('yearmonth' in col) or ('day_of_week' in col) or ('day_of_month') in col or ('day_of_year' in col)]:\n",
    "            traindf[col] = traindf[col].astype(str)\n",
    "            traindf[col] = traindf[col].astype('category')\n",
    "\n",
    "        s = setup(data=traindf, target='target',\n",
    "            transformation=True,\n",
    "            normalize=True,\n",
    "            ignore_features=['customer_ID'],\n",
    "            categorical_features = [col for col in traindf.columns if (col[:len(col)-6] in ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']) or ('yearmonth' in col) or ('day_of_week' in col)],\n",
    "            numeric_features=[col for col in traindf.columns if ('day_of_month') in col or ('day_of_year' in col)],\n",
    "            remove_outliers=True,\n",
    "            handle_unknown_categorical=True,\n",
    "            combine_rare_levels=True,\n",
    "#            create_clusters=True,\n",
    "#            transform_target=True, transform_target_method='yeo-johnson',\n",
    "            trigonometry_features=True,\n",
    "#            polynomial_features=True, polynomial_degree=d,\n",
    "            feature_selection=True, remove_multicollinearity=True, #pca=True,\n",
    "            ignore_low_variance=True,\n",
    "            train_size=0.8, fold=10,\n",
    "            use_gpu=True,\n",
    "            silent = True, session_id = 123, verbose=True)\n",
    "        save_config(root+\"/configs/\" + \"x\" + str(i) + \"config.pkl\")\n",
    "\n",
    "    for j in range(len(algos)):\n",
    "        if loadmodels[j]:\n",
    "            dtmodels.append(load_model(root+\"/models/\" + algos[j] + \"-x\"+str(i)+\"reg\"))\n",
    "        else:\n",
    "            try:\n",
    "                dtmodels.append(create_model(algos[j], fold=10, verbose=True))\n",
    "                save_model(dtmodels[j], root+\"/models/\" + algos[j] + \"-x\"+str(i)+\"reg\")\n",
    "\n",
    "                if algos[j] in ['dt', 'lightgbm']:\n",
    "                    try:\n",
    "                        dtmodels[j] = tune_model(dtmodels[j], fold=10, search_library = \"scikit-optimize\", n_iter=30, verbose=True)\n",
    "                        save_model(dtmodels[j], root+\"/models/\" + algos[j] + \"-x\"+str(i)+\"reg\")\n",
    "                    except:\n",
    "                        print(\"Unable to tune for: \", algos[j])\n",
    "                        traceback.print_exc()\n",
    "            except:\n",
    "                print(\"Unable to build model for:\", algos[j])\n",
    "                traceback.print_exc()\n",
    "\n",
    "##    stacked=stack_models(dtmodels, meta_model=dtmodels[len(dtmodels)-1])\n",
    "##    save_model(stacked, root+\"/models/\" + \"x\" + str(i) + \"stackedreg\")\n",
    "#    try:\n",
    "#        stacked = tune_model(stacked, fold=10, n_iter=15, verbose=True)\n",
    "#        save_model(stacked, root+\"/models/\" + \"x\" + str(i) + \"stackedreg\")\n",
    "#    except:\n",
    "#        print(\"Unable to tune for stacking\")\n",
    "##    dtmodels.append(stacked)\n",
    "    d -= 1 if ((i/3)==round((i/3))) else d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e37c1e0-100d-4689-96f3-c47438e81145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_7dcc9_row10_col0, #T_7dcc9_row10_col1, #T_7dcc9_row10_col2, #T_7dcc9_row10_col3, #T_7dcc9_row10_col4, #T_7dcc9_row10_col5 {\n",
       "  background: yellow;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_7dcc9\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_7dcc9_level0_col0\" class=\"col_heading level0 col0\" >MAE</th>\n",
       "      <th id=\"T_7dcc9_level0_col1\" class=\"col_heading level0 col1\" >MSE</th>\n",
       "      <th id=\"T_7dcc9_level0_col2\" class=\"col_heading level0 col2\" >RMSE</th>\n",
       "      <th id=\"T_7dcc9_level0_col3\" class=\"col_heading level0 col3\" >R2</th>\n",
       "      <th id=\"T_7dcc9_level0_col4\" class=\"col_heading level0 col4\" >RMSLE</th>\n",
       "      <th id=\"T_7dcc9_level0_col5\" class=\"col_heading level0 col5\" >MAPE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Fold</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_7dcc9_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_7dcc9_row0_col0\" class=\"data row0 col0\" >0.1365</td>\n",
       "      <td id=\"T_7dcc9_row0_col1\" class=\"data row0 col1\" >0.0662</td>\n",
       "      <td id=\"T_7dcc9_row0_col2\" class=\"data row0 col2\" >0.2573</td>\n",
       "      <td id=\"T_7dcc9_row0_col3\" class=\"data row0 col3\" >0.5928</td>\n",
       "      <td id=\"T_7dcc9_row0_col4\" class=\"data row0 col4\" >0.1795</td>\n",
       "      <td id=\"T_7dcc9_row0_col5\" class=\"data row0 col5\" >0.3380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7dcc9_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_7dcc9_row1_col0\" class=\"data row1 col0\" >0.1348</td>\n",
       "      <td id=\"T_7dcc9_row1_col1\" class=\"data row1 col1\" >0.0643</td>\n",
       "      <td id=\"T_7dcc9_row1_col2\" class=\"data row1 col2\" >0.2536</td>\n",
       "      <td id=\"T_7dcc9_row1_col3\" class=\"data row1 col3\" >0.5964</td>\n",
       "      <td id=\"T_7dcc9_row1_col4\" class=\"data row1 col4\" >0.1789</td>\n",
       "      <td id=\"T_7dcc9_row1_col5\" class=\"data row1 col5\" >0.3221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7dcc9_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_7dcc9_row2_col0\" class=\"data row2 col0\" >0.1389</td>\n",
       "      <td id=\"T_7dcc9_row2_col1\" class=\"data row2 col1\" >0.0675</td>\n",
       "      <td id=\"T_7dcc9_row2_col2\" class=\"data row2 col2\" >0.2597</td>\n",
       "      <td id=\"T_7dcc9_row2_col3\" class=\"data row2 col3\" >0.5930</td>\n",
       "      <td id=\"T_7dcc9_row2_col4\" class=\"data row2 col4\" >0.1820</td>\n",
       "      <td id=\"T_7dcc9_row2_col5\" class=\"data row2 col5\" >0.3255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7dcc9_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_7dcc9_row3_col0\" class=\"data row3 col0\" >0.1307</td>\n",
       "      <td id=\"T_7dcc9_row3_col1\" class=\"data row3 col1\" >0.0615</td>\n",
       "      <td id=\"T_7dcc9_row3_col2\" class=\"data row3 col2\" >0.2480</td>\n",
       "      <td id=\"T_7dcc9_row3_col3\" class=\"data row3 col3\" >0.6298</td>\n",
       "      <td id=\"T_7dcc9_row3_col4\" class=\"data row3 col4\" >0.1733</td>\n",
       "      <td id=\"T_7dcc9_row3_col5\" class=\"data row3 col5\" >0.3198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7dcc9_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_7dcc9_row4_col0\" class=\"data row4 col0\" >0.1352</td>\n",
       "      <td id=\"T_7dcc9_row4_col1\" class=\"data row4 col1\" >0.0647</td>\n",
       "      <td id=\"T_7dcc9_row4_col2\" class=\"data row4 col2\" >0.2544</td>\n",
       "      <td id=\"T_7dcc9_row4_col3\" class=\"data row4 col3\" >0.5982</td>\n",
       "      <td id=\"T_7dcc9_row4_col4\" class=\"data row4 col4\" >0.1792</td>\n",
       "      <td id=\"T_7dcc9_row4_col5\" class=\"data row4 col5\" >0.3237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7dcc9_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_7dcc9_row5_col0\" class=\"data row5 col0\" >0.1383</td>\n",
       "      <td id=\"T_7dcc9_row5_col1\" class=\"data row5 col1\" >0.0667</td>\n",
       "      <td id=\"T_7dcc9_row5_col2\" class=\"data row5 col2\" >0.2583</td>\n",
       "      <td id=\"T_7dcc9_row5_col3\" class=\"data row5 col3\" >0.6046</td>\n",
       "      <td id=\"T_7dcc9_row5_col4\" class=\"data row5 col4\" >0.1810</td>\n",
       "      <td id=\"T_7dcc9_row5_col5\" class=\"data row5 col5\" >0.3201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7dcc9_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_7dcc9_row6_col0\" class=\"data row6 col0\" >0.1372</td>\n",
       "      <td id=\"T_7dcc9_row6_col1\" class=\"data row6 col1\" >0.0655</td>\n",
       "      <td id=\"T_7dcc9_row6_col2\" class=\"data row6 col2\" >0.2558</td>\n",
       "      <td id=\"T_7dcc9_row6_col3\" class=\"data row6 col3\" >0.6065</td>\n",
       "      <td id=\"T_7dcc9_row6_col4\" class=\"data row6 col4\" >0.1795</td>\n",
       "      <td id=\"T_7dcc9_row6_col5\" class=\"data row6 col5\" >0.3181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7dcc9_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_7dcc9_row7_col0\" class=\"data row7 col0\" >0.1356</td>\n",
       "      <td id=\"T_7dcc9_row7_col1\" class=\"data row7 col1\" >0.0653</td>\n",
       "      <td id=\"T_7dcc9_row7_col2\" class=\"data row7 col2\" >0.2555</td>\n",
       "      <td id=\"T_7dcc9_row7_col3\" class=\"data row7 col3\" >0.6070</td>\n",
       "      <td id=\"T_7dcc9_row7_col4\" class=\"data row7 col4\" >0.1788</td>\n",
       "      <td id=\"T_7dcc9_row7_col5\" class=\"data row7 col5\" >0.3198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7dcc9_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_7dcc9_row8_col0\" class=\"data row8 col0\" >0.1339</td>\n",
       "      <td id=\"T_7dcc9_row8_col1\" class=\"data row8 col1\" >0.0638</td>\n",
       "      <td id=\"T_7dcc9_row8_col2\" class=\"data row8 col2\" >0.2525</td>\n",
       "      <td id=\"T_7dcc9_row8_col3\" class=\"data row8 col3\" >0.6079</td>\n",
       "      <td id=\"T_7dcc9_row8_col4\" class=\"data row8 col4\" >0.1767</td>\n",
       "      <td id=\"T_7dcc9_row8_col5\" class=\"data row8 col5\" >0.3260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7dcc9_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_7dcc9_row9_col0\" class=\"data row9 col0\" >0.1377</td>\n",
       "      <td id=\"T_7dcc9_row9_col1\" class=\"data row9 col1\" >0.0667</td>\n",
       "      <td id=\"T_7dcc9_row9_col2\" class=\"data row9 col2\" >0.2582</td>\n",
       "      <td id=\"T_7dcc9_row9_col3\" class=\"data row9 col3\" >0.6029</td>\n",
       "      <td id=\"T_7dcc9_row9_col4\" class=\"data row9 col4\" >0.1801</td>\n",
       "      <td id=\"T_7dcc9_row9_col5\" class=\"data row9 col5\" >0.3325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7dcc9_level0_row10\" class=\"row_heading level0 row10\" >Mean</th>\n",
       "      <td id=\"T_7dcc9_row10_col0\" class=\"data row10 col0\" >0.1359</td>\n",
       "      <td id=\"T_7dcc9_row10_col1\" class=\"data row10 col1\" >0.0652</td>\n",
       "      <td id=\"T_7dcc9_row10_col2\" class=\"data row10 col2\" >0.2554</td>\n",
       "      <td id=\"T_7dcc9_row10_col3\" class=\"data row10 col3\" >0.6039</td>\n",
       "      <td id=\"T_7dcc9_row10_col4\" class=\"data row10 col4\" >0.1789</td>\n",
       "      <td id=\"T_7dcc9_row10_col5\" class=\"data row10 col5\" >0.3246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_7dcc9_level0_row11\" class=\"row_heading level0 row11\" >Std</th>\n",
       "      <td id=\"T_7dcc9_row11_col0\" class=\"data row11 col0\" >0.0023</td>\n",
       "      <td id=\"T_7dcc9_row11_col1\" class=\"data row11 col1\" >0.0017</td>\n",
       "      <td id=\"T_7dcc9_row11_col2\" class=\"data row11 col2\" >0.0033</td>\n",
       "      <td id=\"T_7dcc9_row11_col3\" class=\"data row11 col3\" >0.0102</td>\n",
       "      <td id=\"T_7dcc9_row11_col4\" class=\"data row11 col4\" >0.0023</td>\n",
       "      <td id=\"T_7dcc9_row11_col5\" class=\"data row11 col5\" >0.0060</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f7050717d60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation Pipeline and Model Successfully Saved\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33b73650cc9241d4b001c7e7d5750037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, description='Processing: ', max=7)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Initiated</th>\n",
       "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
       "      <td>16:05:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Status</th>\n",
       "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
       "      <td>Searching Hyperparameters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Estimator</th>\n",
       "      <td>. . . . . . . . . . . . . . . . . .</td>\n",
       "      <td>Light Gradient Boosting Machine</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                \\\n",
       "                                                 \n",
       "Initiated  . . . . . . . . . . . . . . . . . .   \n",
       "Status     . . . . . . . . . . . . . . . . . .   \n",
       "Estimator  . . . . . . . . . . . . . . . . . .   \n",
       "\n",
       "                                            \n",
       "                                            \n",
       "Initiated                         16:05:07  \n",
       "Status           Searching Hyperparameters  \n",
       "Estimator  Light Gradient Boosting Machine  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fold</th>\n",
       "      <th>MAE</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>R2</th>\n",
       "      <th>RMSLE</th>\n",
       "      <th>MAPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Fold, MAE, MSE, RMSE, R2, RMSLE, MAPE]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "algos=['dt', 'catboost', 'lightgbm']\n",
    "algos=['lightgbm']\n",
    "loadconfig=[False, False, False, False, False]\n",
    "loadmodel=[False, False, False, False, False]\n",
    "tunemodel=[True, True, True, True, True]\n",
    "d=2\n",
    "dtmodels=[]\n",
    "nof=4\n",
    "\n",
    "for i in range(5):\n",
    "    tag = \"\".join([str((i*nof)+j) for j in range(nof)])\n",
    "    if loadconfig[i]:\n",
    "        load_config(root+\"/configs/\" + \"x12-\" + tag + \"config.pkl\")\n",
    "    else:\n",
    "#        print(\"x12-\" + str(i*nof) + \"dtcnt.csv\")\n",
    "#        traindf = pd.read_csv(\"x12-\" + str(i*nof) + \"dtcnt.csv\")\n",
    "        traindf=pd.DataFrame()\n",
    "        for j in range(nof):\n",
    "#            if exists(\"x12-\" + str((i*nof)+j) + \"dtcnt.csv\"):\n",
    "            print(\"x12-\" + str((i*nof)+j) + \"dtcnt.csv\", end=\" \")\n",
    "            traindf = pd.concat([traindf, pd.read_csv(\"x12-\" + str((i*nof)+j) + \"dtcnt.csv\")], ignore_index=True)\n",
    "#            else:\n",
    "#                print(\"x12-\" + str((i*nof)-1) + \"dtcnt.csv\")\n",
    "#                traindf = pd.concat([traindf, pd.read_csv(\"x12-\" + str((i*nof)-1) + \"dtcnt.csv\")], ignore_index=True)\n",
    "\n",
    "#        traindf.to_csv(\"x12-\"+str((i*3))+str((i*3)+1)+str((i*3)+2)+\".csv\")\n",
    "#        print(\"wrote x12-\"+str((i*3))+str((i*3)+1)+str((i*3)+2)+\".csv\")\n",
    "        print(\"Loaded data...\", \"train count:\", traindf.shape, traindf.groupby(['target'])['customer_ID'].count())\n",
    "\n",
    "        for col in [col for col in traindf.columns if (col[:len(col)-6] in ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']) or\n",
    "                    ('yearmonth' in col) or ('day_of_week' in col) or ('day_of_month') in col or ('day_of_year' in col)]:\n",
    "#            traindf[col] = traindf[col].astype(str)\n",
    "            traindf[col] = traindf[col].astype('category')\n",
    "\n",
    "        s = setup(data=traindf, target='target',\n",
    "            transformation=True,\n",
    "            normalize=True,\n",
    "            ignore_features=['customer_ID'],\n",
    "            categorical_features = [col for col in traindf.columns if (col[:len(col)-6] in ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']) or ('yearmonth' in col) or ('day_of_week' in col) or ('day_of_month') in col or ('day_of_year' in col)],\n",
    "#            numeric_features=[col for col in traindf.columns if ('day_of_month') in col or ('day_of_year' in col)],\n",
    "            remove_outliers=True,\n",
    "            handle_unknown_categorical=True,\n",
    "            combine_rare_levels=True,\n",
    "#            create_clusters=True,\n",
    "#            transform_target=True, transform_target_method='yeo-johnson',\n",
    "#            trigonometry_features=True,\n",
    "#            polynomial_features=True, polynomial_degree=d,\n",
    "            feature_selection=True, feature_selection_threshold=0.9,\n",
    "#            remove_multicollinearity=True, #pca=True,\n",
    "            ignore_low_variance=True,\n",
    "            train_size=0.8, fold=10,\n",
    "            use_gpu=True,\n",
    "            silent = True, session_id = 123, verbose=True)\n",
    "        traindf=pd.DataFrame() #to free up the memory\n",
    "        save_config(root+\"/configs/\" + \"x12-\" + tag + \"config.pkl\")\n",
    "\n",
    "    for j in range(len(algos)):\n",
    "        if loadmodel[j]:\n",
    "            dtmodels.append(load_model(root+\"/models/\" + algos[j] + \"-x12-\"+tag+\"reg\"))\n",
    "        else:\n",
    "            try:\n",
    "                dtmodels.append(create_model(algos[j], fold=10, verbose=True))\n",
    "                save_model(dtmodels[j], root+\"/models/\" + algos[j] + \"-x12-\" + tag+\"reg\")\n",
    "            except:\n",
    "                print(\"Unable to build model for:\", algos[j])\n",
    "                traceback.print_exc()\n",
    "        if tunemodel[j]:\n",
    "            if algos[j] in ['dt', 'lightgbm']:\n",
    "                try:\n",
    "                    dtmodels[j] = tune_model(dtmodels[j], fold=10, search_library = \"scikit-optimize\", n_iter=15, verbose=True)\n",
    "                    save_model(dtmodels[j], root+\"/models/\" + algos[j] + \"-x12-\"+ tag +\"reg\")\n",
    "                except:\n",
    "                    print(\"Unable to tune for: \", algos[j])\n",
    "                    traceback.print_exc()\n",
    "\n",
    "##stacked=stack_models(dtmodels, meta_model=dtmodels[len(dtmodels)-1])\n",
    "##save_model(stacked, root+\"/models/\" + \"x12-\" + str(i) + \"stackedreg\")\n",
    "#    try:\n",
    "#        stacked = tune_model(stacked, fold=10, n_iter=15, verbose=True)\n",
    "#        save_model(stacked, root+\"/models/\" + \"x\" + str(i) + \"stackedreg\")\n",
    "#    except:\n",
    "#        print(\"Unable to tune for stacking\")\n",
    "##dtmodels.append(stacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189581fb-f4d5-4828-9304-c2ae32a11e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtmodels[0] = tune_model(dtmodels[0], fold=10, search_library = \"scikit-optimize\", n_iter=50, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad68c46-838a-4ee6-9c42-0237c0eaa841",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf = pd.read_csv(root+\"/xall.csv\")\n",
    "s = setup(data=traindf, target='target',\n",
    "    transformation=True,\n",
    "    normalize=True,\n",
    "    ignore_features=['customer_ID'],\n",
    "    categorical_features = [col for col in traindf.columns if (col[:len(col)-6] in ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']) or ('yearmonth' in col) or ('day_of_week' in col)],\n",
    "    numeric_features=[col for col in traindf.columns if ('day_of_month') in col or ('day_of_year' in col)],\n",
    "#            remove_outliers=True,\n",
    "    handle_unknown_categorical=True,\n",
    "#            combine_rare_levels=True,\n",
    "#            create_clusters=True,\n",
    "#            transform_target=True, transform_target_method='yeo-johnson',\n",
    "#    trigonometry_features=True,\n",
    "#            polynomial_features=True, polynomial_degree=d,\n",
    "    feature_selection=True, remove_multicollinearity=True, #pca=True,\n",
    "    ignore_low_variance=True,\n",
    "    train_size=0.9, fold=10,\n",
    "    use_gpu=True,\n",
    "    silent = True, session_id = 123, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dda866-0b52-465b-9b11-1e05ee1f17ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model('lightgbm', verbose=True)\n",
    "evaluate_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7bf4af-4969-4d71-a8ad-2bf38f4e97a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned = tune_model(model, search_library = \"scikit-optimize\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcd79e9-f7d2-4b85-abad-e3135ef6da5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29c010f-ecd3-4158-a60d-7673eeb69098",
   "metadata": {},
   "outputs": [],
   "source": [
    "blend = blend_models(dtmodels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba782755-8892-475c-9fba-e6df2be93160",
   "metadata": {},
   "outputs": [],
   "source": [
    "finmodel = finalize_model(blend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f429c7-7fb3-413f-91c3-086e4a63cd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, root+\"/models/lightgbm-xall\")\n",
    "plot_model(model, 'error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46132ab8-6139-4eb2-9e18-e8657805fec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(tuned, root+\"/models/tuned-lightgbm-xall\")\n",
    "plot_model(tuned, 'error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ed6fc6-31c6-4c2b-8d0a-8bb490b679b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtmodels=[]\n",
    "dtsets=[]\n",
    "headers=pd.read_csv(root+\"/trains/headers.csv\")\n",
    "headers = headers.columns\n",
    "n=18\n",
    "algos=['dt', 'catboost', 'lightgbm']\n",
    "\n",
    "for i in range(1):\n",
    "#    for algo in algos:\n",
    "#        if algo == \"catboost\":\n",
    "#            dtmodels.append(load_model(root+\"/models/\" + algo + str(n)+\"-\"+str(i)+\"reg\"))\n",
    "#    dtmodels.append(load_model(root+\"/models/\" + \"\".join(algos) + str(n) + \"-\" + str(i) + \"stackedreg\"))\n",
    "    dtmodels.append(load_model(root+\"/models/\" + \"x\" + str(i) + \"stackedreg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5015f2fa-3ea3-4824-93d3-b038cb348094",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked=stack_models(dtmodels, meta_model=dtmodels[0])\n",
    "#stacked = tune_model(stacked, fold=10, n_iter=15, verbose=True)\n",
    "dtmodels.append(stacked)\n",
    "save_model(stacked, root+\"/models/\" + \"x\" + str(i) + \"stackedreg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f70d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = []\n",
    "for r, d, f in os.walk(root+\"test\"):\n",
    "#    print(r)\n",
    "#    if r ==root+'/test':\n",
    "        f.sort()\n",
    "        for file in f:\n",
    "            if '.idx' not in file and '.csv' not in file and 'cust' not in file and 'preds' not in file and ('testa' in file or 'testb' in file or 'testc' in file or 'testd' in file or 'teste' in file or 'testf' in file):\n",
    "                tests.append(os.path.join(r, file))\n",
    "len(tests)#, tests[len(tests)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f23246-cd6b-43cc-92bd-6ede1e4ce46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#traindf = pd.concat(traindfs, ignore_index=True)\n",
    "headers=pd.read_csv(root+\"headers.csv\")\n",
    "headers = headers.columns\n",
    "\n",
    "testdf = pd.read_csv(root+\"/tstdata\")\n",
    "tdf = testdf.groupby('customer_ID')['S_2'].agg(['min', 'max', 'count']).reset_index()\n",
    "for i in range(1, 14):\n",
    "    print(i, tdf[tdf['count']==i]['count'].count())\n",
    "\n",
    "tdf.rename(columns={\"min\": \"mindate\", \"max\": \"maxdate\"}, inplace=True)\n",
    "\n",
    "tdf['maxdate'] = pd.to_datetime(tdf['maxdate'])\n",
    "tdf['mindate'] = pd.to_datetime(tdf['mindate'])\n",
    "\n",
    "#tdf['diffcount'] = tdf.apply(lambda x: ((x['maxdate'] - x['mindate'])/np.timedelta64(1, 'M')))\n",
    "\n",
    "tdf['mondiff'] = (tdf.maxdate.dt.strftime(\"%Y%m\").astype(int) - tdf.mindate.dt.strftime(\"%Y%m\").astype(int) - 87)\n",
    "tdf['mondiff'] = tdf['mondiff'].apply(lambda x: x+88 if x< 0 else x)\n",
    "tdf[tdf['count'] != tdf['mondiff']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a973a321-f6df-4e18-b7f6-ea3b7aeaa89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "testdfs = []\n",
    "\n",
    "for k in range(13):\n",
    "    testdfs.append(pd.DataFrame(columns=headers))\n",
    "\n",
    "k=0\n",
    "for i in range(len(tests)):\n",
    "    print(tests[i])\n",
    "    testdf = pd.read_csv(tests[i], names=headers, header=None)\n",
    "    for j in range(1, 14):\n",
    "        mask = testdf.customer_ID.isin(tdf[tdf['mondiff'] == j].customer_ID)\n",
    "        testdfs[j-1] = pd.concat([testdfs[j-1], testdf[mask]], ignore_index=True)\n",
    "        if testdfs[j-1]['customer_ID'].count() >= 260000:\n",
    "            testdfs[j-1].iloc[:260000,].to_csv(\"t\" + str(j-1) + \"-\" + str(k) + \".csv\", index=False)\n",
    "            print(\"t\" + str(i) + \"-\" + str(k) + \".csv\")\n",
    "            testdfs[j-1] = testdfs[j-1].iloc[260000:,]\n",
    "            k+=1\n",
    "\n",
    "for i in range(12):\n",
    "    testdfs[i].to_csv(\"t\" + str(i) + \".csv\", index=False)\n",
    "    print(\"t\" + str(i) + \".csv\")\n",
    "\n",
    "for i in range(12, len(testdfs)):\n",
    "    testdfs[i].to_csv(\"t\" + str(i) + \"-\" + str(k) + \".csv\", index=False)\n",
    "    print(\"t\" + str(i) + \"-\" + str(k) + \".csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e29f64d-7eeb-45e8-ab56-c30d7ccdb502",
   "metadata": {},
   "outputs": [],
   "source": [
    "testdfs = []\n",
    "\n",
    "for k in range(13):\n",
    "    testdfs.append(pd.DataFrame(columns=headers))\n",
    "k=0\n",
    "for i in range(len(tests)):\n",
    "    print(tests[i])\n",
    "    testdf = pd.read_csv(tests[i], names=headers, header=None)\n",
    "    for j in range(13, 14):\n",
    "        mask = testdf.customer_ID.isin(tdf[tdf['S_2']==j].customer_ID)\n",
    "        testdfs[j-1] = pd.concat([testdfs[j-1], testdf[mask]], ignore_index=True)\n",
    "        if testdfs[j-1]['customer_ID'].count() >= 260000:\n",
    "            testdfs[j-1].iloc[:260000,].to_csv(\"t\" + str(j-1) + \"-\" + str(k) + \".csv\", index=False)\n",
    "            testdfs[j-1] = testdfs[j-1].iloc[260000:,]\n",
    "            k+=1            \n",
    "\n",
    "testdfs[12].to_csv(\"t12-\" + str(k) + \".csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dc7126-8f31-4205-9396-e79b00191e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=pd.DataFrame()\n",
    "for i in range(11, 12): #, 12):\n",
    "    model = dtmodels[len(dtmodels)-1] #load_model(root+\"/models/\" + \"x\" + str(i) + \"stackedreg\")\n",
    "    print(\"t\" + str(i) + \"dtcnt.csv\")\n",
    "    testdf = (pd.read_csv(\"t\" + str(i) + \"dtcnt.csv\"))\n",
    "\n",
    "    preds = predict_model(model, data=testdf, verbose=True)\n",
    "    predictions = pd.concat([predictions, preds], ignore_index=True, axis=0)\n",
    "    preds[['customer_ID', 'Label']].to_csv(root+\"/test/t\"+ str(i) + \".dtcnt\" + \".preds\", header=False, index=False)\n",
    "\n",
    "#predictions.to_csv(\"test.preds\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57716a7c-4d84-4742-88ea-0e498b7fe382",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testdfs = []\n",
    "i=0\n",
    "#mdf = pd.DataFrame()\n",
    "#for m in range(2):\n",
    "#    mdf = pd.concat([mdf, pd.read_csv(\"t12-\"+str(m)+\".csv\")], ignore_index=True)\n",
    "#mdf['tag'] = 12\n",
    "#mdf['S_2'] = pd.to_datetime(mdf['S_2'])\n",
    "for j in range(12):\n",
    "    print(\"t\" + str(j), end=\" \")\n",
    "    testdfs.append(pd.read_csv(\"t\" + str(j) + \".csv\"))\n",
    "\n",
    "    maxdate = datetime.strptime('Apr 30 2019 1:33PM', '%b %d %Y %I:%M%p')\n",
    "    mindate = maxdate + pd.offsets.DateOffset(months= -1-j)\n",
    "    dates=pd.DataFrame(pd.date_range(mindate, maxdate, freq='MS').tolist(), columns=['S_2_dt'])\n",
    "    dates['yearmonth'] = dates['S_2_dt'].dt.strftime(\"%Y%m\")\n",
    "    dates.reset_index(inplace=True)\n",
    "    dates.rename(columns={'index': 'seq'}, inplace=True)\n",
    "    dates['seq']=dates['seq']+1\n",
    "\n",
    "    testdfs[i]['S_2'] = pd.to_datetime(testdfs[i]['S_2'])\n",
    "    testdfs[i]['yearmonthorig'] = testdfs[i]['S_2'].dt.strftime(\"%Y%m\")\n",
    "    testdfs[i]['seq'] = testdfs[i].groupby(['customer_ID']).cumcount().add(1)\n",
    "    testdfs[i][['sncnt', 'dncnt', 'rncnt', 'bncnt', 'pncnt']] = testdfs[i].apply(lambda x: (sum([(2^scols.index(col)) for col in scols if pd.isnull(x[col])]),\n",
    "                sum([(2^dcols.index(col)) for col in dcols if pd.isnull(x[col])]),\n",
    "                sum([(2^rcols.index(col)) for col in rcols if pd.isnull(x[col])]),\n",
    "                sum([(2^bcols.index(col)) for col in bcols if pd.isnull(x[col])]),\n",
    "                sum([(2^pcols.index(col)) for col in pcols if pd.isnull(x[col])])), axis=1, result_type='expand')\n",
    "\n",
    "    dateshift = pd.DataFrame(testdfs[i].groupby(['customer_ID'])['S_2'].agg(['min', 'max', 'count'])).rename(columns={\"max\": \"S_2\"}).reset_index()\n",
    "    dateshift['S_2'] = pd.to_datetime(dateshift['S_2'])\n",
    "    dateshift['mondiff'] = dateshift['S_2'].dt.strftime(\"%Y%m\").astype(int) - (maxdate.year*100 + maxdate.month)\n",
    "\n",
    "#    dateshift['mondiff'] = dateshift['S_2'].dt.month - 4\n",
    "#    dateshift['mondiff'] = dateshift.apply(lambda x: 12+x['mondiff'] if x['S_2'].year == 2019 and x['mondiff'] > j else x['mondiff'], axis=1)\n",
    "#    dateshift['mondiff'] = dateshift['mondiff'].apply(lambda x: 12+x if x <=0 else x)\n",
    "\n",
    "    testdfs[i] = pd.merge(testdfs[i], dateshift[['customer_ID', 'mondiff']], on=['customer_ID'], how='inner')\n",
    "    testdfs[i]['S_2'] = testdfs[i].apply(lambda x: x['S_2'] - pd.offsets.DateOffset(months=x['mondiff']), axis=1)\n",
    "    testdfs[i]['S_2'] = testdfs[i].apply(lambda x: x['S_2'] - pd.offsets.DateOffset(years=1) if x['S_2'].year==2019 and x['S_2'].month > 4 else x['S_2'], axis=1)\n",
    "    testdfs[i]['yearmonth'] = testdfs[i]['S_2'].dt.strftime(\"%Y%m\")\n",
    "\n",
    "#    testdfs[i]['ncount'] = testdfs[i].isnull().sum(axis=1)\n",
    "\n",
    "    testdfs[i]=pd.merge(testdfs[i], pd.merge(pd.DataFrame(testdfs[i]['customer_ID'].unique(), columns=['customer_ID']), dates, how='cross'), on=['customer_ID', 'yearmonth'], how='outer')\n",
    "#    testdfs[i]['origmissrcnt'] = testdfs[i].apply(lambda x: (2**(x['seq_y']-1)) if (pd.isnull(x['S_2']) and x['seq_y'] <=j) else 0, axis=1)\n",
    "#    testdfs[i]['totmissrcnt'] = testdfs[i].apply(lambda x: (2**(x['seq_y']-1)) if (pd.isnull(x['S_2'])) else 0, axis=1)\n",
    "    testdfs[i]['origmissrcnt'] = testdfs[i].apply(lambda x: 1 if (pd.isnull(x['S_2']) and x['seq_y'] <=j) else 0, axis=1)\n",
    "    testdfs[i]['totmissrcnt'] = testdfs[i].apply(lambda x: 1 if (pd.isnull(x['S_2'])) else 0, axis=1)\n",
    "    dateshift = pd.merge(dateshift, pd.DataFrame(testdfs[i].groupby(['customer_ID'])['origmissrcnt'].sum()).reset_index(), on ='customer_ID', how='inner')\n",
    "    dateshift = pd.merge(dateshift, pd.DataFrame(testdfs[i].groupby(['customer_ID'])['totmissrcnt'].sum()).reset_index(), on ='customer_ID', how='inner')\n",
    "    testdfs[i]['S_2'] = testdfs[i].apply(lambda x: x['S_2_dt'] if (pd.isnull(x['S_2']) or x['S_2'].month != x['S_2_dt'].month) else x['S_2'], axis=1)\n",
    "    testdfs[i].drop(columns=['S_2_dt', 'seq_x', 'seq_y'], inplace=True)\n",
    "#    testdfs[i] = testdfs[i].rename(columns={\"yearmonth_x\": \"yearmonth\"})\n",
    "\n",
    "    testdfs[i]['yearmonthorig'].fillna(0, inplace=True)\n",
    "#    testdfs[i]['mondiff'].fillna(0, inplace=True)\n",
    "    testdfs[i]['day_of_month'] = testdfs[i]['S_2'].dt.strftime(\"%d\")\n",
    "    testdfs[i]['day_of_week'] = testdfs[i]['S_2'].dt.strftime(\"%u\")\n",
    "    testdfs[i]['day_of_year'] = testdfs[i]['S_2'].dt.strftime(\"%j\")\n",
    "    testdfs[i]['week_of_year'] = testdfs[i]['S_2'].dt.isocalendar().week\n",
    "    testdfs[i].drop(columns=['mondiff'], inplace=True)\n",
    "#    testdfs[i].drop(columns=['S_2'], inplace=True)\n",
    "\n",
    "#    testdfs[i]['tag'] = 0\n",
    "#    testdfs[i] = pd.concat([testdfs[i], mdf], ignore_index=True)\n",
    "\n",
    "#    testdfs[i]['day_of_month'] = testdfs[i]['S_2'].dt.strftime(\"%d\")\n",
    "#    testdfs[i]['day_of_week'] = testdfs[i]['S_2'].dt.strftime(\"%u\")\n",
    "#    testdfs[i]['day_of_year'] = testdfs[i]['S_2'].dt.strftime(\"%j\")\n",
    "#    testdfs[i]['week_of_year'] = testdfs[i]['S_2'].dt.isocalendar().week\n",
    "    testdfs[i].drop(columns=['S_2'], inplace=True)\n",
    "\n",
    "#    cols=testdfs[i][testdfs[i].select_dtypes(include=['int64', 'float64']).columns].isna().any().index\n",
    "#    cols = [col for col in cols if col[0:1] in ['S','R', 'B', 'D', 'P']]\n",
    "#    testdfs[i][cols] = testdfs[i].groupby(['day_of_year', 'day_of_week'])[cols].transform(lambda x : x.fillna(x.iloc[np.random.choice(range(0,len(x)))]))\n",
    "#    testdfs[i][cols] = testdfs[i].groupby(['day_of_year'])[cols].transform(lambda x : x.fillna(x.iloc[np.random.choice(range(0,len(x)))]))\n",
    "\n",
    "#    testdfs[i][cols] = testdfs[i].groupby(['day_of_month', 'day_of_week'])[cols].transform(lambda x : x.fillna(x.iloc[np.random.choice(range(0,len(x)))]))\n",
    "#    testdfs[i][cols] = testdfs[i].groupby(['day_of_month'])[cols].transform(lambda x : x.fillna(x.iloc[np.random.choice(range(0,len(x)))]))\n",
    "\n",
    "#    testdfs[i][cols] = testdfs[i].groupby(['yearmonth', 'day_of_week'])[cols].transform(lambda x : x.fillna(x.iloc[np.random.choice(range(0,len(x)))]))\n",
    "#    testdfs[i][cols] = testdfs[i].groupby(['yearmonth'])[cols].transform(lambda x : x.fillna(x.iloc[np.random.choice(range(0,len(x)))]))\n",
    "\n",
    "#    testdfs[i][cols] = testdfs[i].groupby(['day_of_week'])[cols].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "#    testdfs[i].drop(testdfs[i][testdfs[i]['tag']==12].index, axis=0, inplace=True)\n",
    "#    testdfs[i].drop(columns=['tag'], inplace=True)\n",
    "\n",
    "    testdfs[i]=testdfs[i].set_index(['customer_ID', 'yearmonth']).unstack(1).sort_index(axis=1, level=1)\n",
    "    testdfs[i] = testdfs[i].reset_index()\n",
    "\n",
    "    testdfs[i] = pd.merge(testdfs[i], dateshift[['customer_ID', 'origmissrcnt', 'totmissrcnt']], on=['customer_ID'], how='inner')\n",
    "\n",
    "#    testdfs[i].fillna(0, inplace=True)\n",
    "\n",
    "#    for col in testdfs[i][testdfs[i].select_dtypes(include=['int64', 'float64']).columns].isna().any().index:\n",
    "#        testdfs[i][col].fillna(testdfs[i][[column for column in testdfs[i].columns.tolist() if col[:len(col)-3] == column[:len(column)-3]]].mean(axis=1), inplace=True)\n",
    "\n",
    "#    for col in testdfs[i][testdfs[i].select_dtypes(include=['int64', 'float64']).columns].isna().any().index:\n",
    "#        testdfs[i][col].fillna(testdfs[i][col].mean(), inplace=True)\n",
    "\n",
    "#    for col in testdfs[i][testdfs[i].select_dtypes(include=['int64', 'float64']).columns].isna().any().index:\n",
    "#        testdfs[i][col].fillna(-999, inplace=True)\n",
    "\n",
    "    testdfs[i].drop(columns=[testdfs[i].columns.to_list()[1]], inplace=True)\n",
    "\n",
    "    coldict={}\n",
    "    for col in testdfs[i].columns:\n",
    "        coldict[col] = ''.join(col)\n",
    "    testdfs[i].rename(coldict, axis=1, inplace=True)\n",
    "\n",
    "    testdfs[i].to_csv(\"t\" + str(j) +\"13dts.csv\", index=False)\n",
    "    i+=1\n",
    "\n",
    "testdfs[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ca0e7e-9099-465b-a702-d805b97f1080",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf=pd.read_csv(\"mdfdtcnt.csv\")\n",
    "for j in range(12):\n",
    "    print(j, end=\" \")\n",
    "    pd.concat([mdf, traindfs[j]], ignore_index=True).to_csv(\"x\" + str(j) + \"13dts.csv\", index=False)\n",
    "    pd.concat([mdf[mdf.columns.tolist()[:len(mdf.columns)-2]], testdfs[j]], ignore_index=True).to_csv(\"t\" + str(j) + \"13dts.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2132d850-e81a-48a8-9643-035fec327ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t12-0.csv\n",
      "t12-1.csv\n",
      "t12-2.csv\n",
      "t12-3.csv\n",
      "t12-4.csv\n",
      "t12-5.csv\n"
     ]
    }
   ],
   "source": [
    "testdfs = []\n",
    "i=0\n",
    "maxdate = datetime.strptime('Apr 30 2019 1:33PM', '%b %d %Y %I:%M%p')\n",
    "mindate = maxdate + pd.offsets.DateOffset(months= -13)\n",
    "dates=pd.DataFrame(pd.date_range(mindate, maxdate, freq='MS').tolist(), columns=['S_2_dt'])\n",
    "dates['yearmonth'] = dates['S_2_dt'].dt.strftime(\"%Y%m\")\n",
    "dates.reset_index(inplace=True)\n",
    "dates.rename(columns={'index': 'seq'}, inplace=True)\n",
    "dates['seq']=dates['seq']+1\n",
    "\n",
    "for j in range(41):\n",
    "    print(\"t12-\" + str(j) + \".csv\")\n",
    "    testdfs.append(pd.read_csv(\"t12-\" + str(j) + \".csv\"))\n",
    "\n",
    "    testdfs[i]['S_2'] = pd.to_datetime(testdfs[i]['S_2'])\n",
    "    testdfs[i]['yearmonthorig'] = testdfs[i]['S_2'].dt.strftime(\"%Y%m\")\n",
    "    testdfs[i]['seq'] = testdfs[i].groupby(['customer_ID']).cumcount().add(1)\n",
    "    testdfs[i][['sncnt', 'dncnt', 'rncnt', 'bncnt', 'pncnt']] = testdfs[i].apply(lambda x: (sum([(2^scols.index(col)) for col in scols if pd.isnull(x[col])]),\n",
    "                sum([(2^dcols.index(col)) for col in dcols if pd.isnull(x[col])]),\n",
    "                sum([(2^rcols.index(col)) for col in rcols if pd.isnull(x[col])]),\n",
    "                sum([(2^bcols.index(col)) for col in bcols if pd.isnull(x[col])]),\n",
    "                sum([(2^pcols.index(col)) for col in pcols if pd.isnull(x[col])])), axis=1, result_type='expand')\n",
    "\n",
    "    dateshift = pd.DataFrame(testdfs[i].groupby(['customer_ID'])['S_2'].agg(['min', 'max', 'count'])).rename(columns={\"max\": \"S_2\"}).reset_index()\n",
    "    dateshift['S_2'] = pd.to_datetime(dateshift['S_2'])\n",
    "    dateshift['mondiff'] = dateshift['S_2'].dt.strftime(\"%Y%m\").astype(int) - (maxdate.year*100 + maxdate.month)\n",
    "\n",
    "#    dateshift['mondiff'] = dateshift['S_2'].dt.month - 4\n",
    "#    dateshift['mondiff'] = dateshift.apply(lambda x: 12+x['mondiff'] if x['S_2'].year == 2019 and x['mondiff'] > j else x['mondiff'], axis=1)\n",
    "#    dateshift['mondiff'] = dateshift['mondiff'].apply(lambda x: 12+x if x <=0 else x)\n",
    "\n",
    "    testdfs[i] = pd.merge(testdfs[i], dateshift[['customer_ID', 'mondiff']], on=['customer_ID'], how='inner')\n",
    "    testdfs[i]['S_2'] = testdfs[i].apply(lambda x: x['S_2'] - pd.offsets.DateOffset(months=x['mondiff']), axis=1)\n",
    "    testdfs[i]['S_2'] = testdfs[i].apply(lambda x: x['S_2'] - pd.offsets.DateOffset(years=1) if x['S_2'].year==2019 and x['S_2'].month > 4 else x['S_2'], axis=1)\n",
    "    testdfs[i]['yearmonth'] = testdfs[i]['S_2'].dt.strftime(\"%Y%m\")\n",
    "\n",
    "#    testdfs[i]['ncount'] = testdfs[i].isnull().sum(axis=1)\n",
    "\n",
    "    testdfs[i]=pd.merge(testdfs[i], pd.merge(pd.DataFrame(testdfs[i]['customer_ID'].unique(), columns=['customer_ID']), dates, how='cross'), on=['customer_ID', 'yearmonth'], how='outer')\n",
    "#    testdfs[i]['origmissrcnt'] = testdfs[i].apply(lambda x: (2**(x['seq_y']-1)) if (pd.isnull(x['S_2']) and x['seq_y'] <=j) else 0, axis=1)\n",
    "#    testdfs[i]['totmissrcnt'] = testdfs[i].apply(lambda x: (2**(x['seq_y']-1)) if (pd.isnull(x['S_2'])) else 0, axis=1)\n",
    "    testdfs[i]['origmissrcnt'] = testdfs[i].apply(lambda x: 1 if (pd.isnull(x['S_2']) and x['seq_y'] <=j) else 0, axis=1)\n",
    "    testdfs[i]['totmissrcnt'] = testdfs[i].apply(lambda x: 1 if (pd.isnull(x['S_2'])) else 0, axis=1)\n",
    "    dateshift = pd.merge(dateshift, pd.DataFrame(testdfs[i].groupby(['customer_ID'])['origmissrcnt'].sum()).reset_index(), on ='customer_ID', how='inner')\n",
    "    dateshift = pd.merge(dateshift, pd.DataFrame(testdfs[i].groupby(['customer_ID'])['totmissrcnt'].sum()).reset_index(), on ='customer_ID', how='inner')\n",
    "    testdfs[i]['S_2'] = testdfs[i].apply(lambda x: x['S_2_dt'] if (pd.isnull(x['S_2']) or x['S_2'].month != x['S_2_dt'].month) else x['S_2'], axis=1)\n",
    "    testdfs[i].drop(columns=['S_2_dt', 'seq_x', 'seq_y'], inplace=True)\n",
    "#    testdfs[i] = testdfs[i].rename(columns={\"yearmonth_x\": \"yearmonth\"})\n",
    "\n",
    "    testdfs[i]['yearmonthorig'].fillna(0, inplace=True)\n",
    "#    testdfs[i]['mondiff'].fillna(0, inplace=True)\n",
    "    testdfs[i]['day_of_month'] = testdfs[i]['S_2'].dt.strftime(\"%d\")\n",
    "    testdfs[i]['day_of_week'] = testdfs[i]['S_2'].dt.strftime(\"%u\")\n",
    "    testdfs[i]['day_of_year'] = testdfs[i]['S_2'].dt.strftime(\"%j\")\n",
    "    testdfs[i]['week_of_year'] = testdfs[i]['S_2'].dt.isocalendar().week\n",
    "    testdfs[i].drop(columns=['mondiff'], inplace=True)\n",
    "#    testdfs[i].drop(columns=['S_2'], inplace=True)\n",
    "\n",
    "#    testdfs[i]['tag'] = 0\n",
    "#    testdfs[i] = pd.concat([testdfs[i], mdf], ignore_index=True)\n",
    "\n",
    "#    testdfs[i]['day_of_month'] = testdfs[i]['S_2'].dt.strftime(\"%d\")\n",
    "#    testdfs[i]['day_of_week'] = testdfs[i]['S_2'].dt.strftime(\"%u\")\n",
    "#    testdfs[i]['day_of_year'] = testdfs[i]['S_2'].dt.strftime(\"%j\")\n",
    "#    testdfs[i]['week_of_year'] = testdfs[i]['S_2'].dt.isocalendar().week\n",
    "    testdfs[i].drop(columns=['S_2'], inplace=True)\n",
    "\n",
    "#    cols=testdfs[i][testdfs[i].select_dtypes(include=['int64', 'float64']).columns].isna().any().index\n",
    "#    cols = [col for col in cols if col[0:1] in ['S','R', 'B', 'D', 'P']]\n",
    "#    testdfs[i][cols] = testdfs[i].groupby(['day_of_year', 'day_of_week'])[cols].transform(lambda x : x.fillna(x.iloc[np.random.choice(range(0,len(x)))]))\n",
    "#    testdfs[i][cols] = testdfs[i].groupby(['day_of_year'])[cols].transform(lambda x : x.fillna(x.iloc[np.random.choice(range(0,len(x)))]))\n",
    "\n",
    "#    testdfs[i][cols] = testdfs[i].groupby(['day_of_month', 'day_of_week'])[cols].transform(lambda x : x.fillna(x.iloc[np.random.choice(range(0,len(x)))]))\n",
    "#    testdfs[i][cols] = testdfs[i].groupby(['day_of_month'])[cols].transform(lambda x : x.fillna(x.iloc[np.random.choice(range(0,len(x)))]))\n",
    "\n",
    "#    testdfs[i][cols] = testdfs[i].groupby(['yearmonth', 'day_of_week'])[cols].transform(lambda x : x.fillna(x.iloc[np.random.choice(range(0,len(x)))]))\n",
    "#    testdfs[i][cols] = testdfs[i].groupby(['yearmonth'])[cols].transform(lambda x : x.fillna(x.iloc[np.random.choice(range(0,len(x)))]))\n",
    "\n",
    "#    testdfs[i][cols] = testdfs[i].groupby(['day_of_week'])[cols].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "#    testdfs[i].drop(testdfs[i][testdfs[i]['tag']==12].index, axis=0, inplace=True)\n",
    "#    testdfs[i].drop(columns=['tag'], inplace=True)\n",
    "\n",
    "    testdfs[i]=testdfs[i].set_index(['customer_ID', 'yearmonth']).unstack(1).sort_index(axis=1, level=1)\n",
    "    testdfs[i] = testdfs[i].reset_index()\n",
    "\n",
    "    testdfs[i] = pd.merge(testdfs[i], dateshift[['customer_ID', 'origmissrcnt', 'totmissrcnt']], on=['customer_ID'], how='inner')\n",
    "\n",
    "#    testdfs[i].fillna(0, inplace=True)\n",
    "\n",
    "#    for col in testdfs[i][testdfs[i].select_dtypes(include=['int64', 'float64']).columns].isna().any().index:\n",
    "#        testdfs[i][col].fillna(testdfs[i][[column for column in testdfs[i].columns.tolist() if col[:len(col)-3] == column[:len(column)-3]]].mean(axis=1), inplace=True)\n",
    "\n",
    "#    for col in testdfs[i][testdfs[i].select_dtypes(include=['int64', 'float64']).columns].isna().any().index:\n",
    "#        testdfs[i][col].fillna(testdfs[i][col].mean(), inplace=True)\n",
    "\n",
    "#    for col in testdfs[i][testdfs[i].select_dtypes(include=['int64', 'float64']).columns].isna().any().index:\n",
    "#        testdfs[i][col].fillna(-999, inplace=True)\n",
    "\n",
    "    testdfs[i].drop(columns=[testdfs[i].columns.to_list()[1]], inplace=True)\n",
    "\n",
    "    coldict={}\n",
    "    for col in testdfs[i].columns:\n",
    "        coldict[col] = ''.join(col)\n",
    "    testdfs[i].rename(coldict, axis=1, inplace=True)\n",
    "\n",
    "    testdfs[i].to_csv(\"t12-\" + str(j) +\"dtcnt.csv\", index=False)\n",
    "#    os.remove(tests[i]\n",
    "    i+=1\n",
    "\n",
    "testdfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333cc436-d480-4af5-a03c-3e53c5e22504",
   "metadata": {},
   "outputs": [],
   "source": [
    "testdfs[0]\n",
    "#dateshift.sort_values('mondiff', ascending=False)\n",
    "#dateshift = pd.DataFrame(testdfs[i].groupby(['customer_ID'])['S_2'].agg(['min', 'max', 'count'])).rename(columns={\"max\": \"S_2\"}).reset_index()\n",
    "#dateshift['min'] = pd.to_datetime(dateshift['min'])\n",
    "#dateshift['S_2'] = pd.to_datetime(dateshift['S_2'])\n",
    "\n",
    "#dateshift['mondiff'] = dateshift['S_2'].dt.strftime(\"%Y%m\").astype(int) - dateshift['min'].dt.strftime(\"%Y%m\").astype(int) - 87\n",
    "#dateshift[dateshift['customer_ID'] == '064b1f0072facb6599248737d0b1c9f011eba59233794057beb66b1dc13d647a']\n",
    "\n",
    "testdfs[0][testdfs[i]['customer_ID'] == '064b1f0072facb6599248737d0b1c9f011eba59233794057beb66b1dc13d647a']\n",
    "\n",
    "#pd.DataFrame(testdfs[i].groupby(['customer_ID'])['S_2'].agg(['min', 'max', 'count'])).rename(columns={\"max\": \"S_2\"}).reset_index()\n",
    "\n",
    "testdfs[i]['yearmonth'] = testdfs[i]['S_2'].dt.strftime(\"%Y%m\")\n",
    "tst = testdfs[i].set_index(['customer_ID', 'yearmonth']).unstack(1).sort_index(axis=1, level=1).reset_index()\n",
    "#testdfs[i][testdfs[i]['customer_ID'] == '064aee43581981bdcf7e2906f8cb875a503445ed7ac2d40ad68a03c213398e79']\n",
    "#tst[tst['customer_ID'] == '064aee43581981bdcf7e2906f8cb875a503445ed7ac2d40ad68a03c213398e79']\n",
    "\n",
    "dateshift\n",
    "\n",
    "dateshift['S_2'].dt.strftime(\"%Y%m\").astype(int) - (maxdate.year*100 + maxdate.month) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5f9a41",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model = load_model(\"/amex/models/dt1Lclass\")\n",
    "#model = calibrated_dt\n",
    "#model=stacked\n",
    "#model=load_model(\"/amex/models/dtstackedreg\")\n",
    "\n",
    "useFlag = False\n",
    "mindate = datetime.strptime('Apr 1 2018  1:33PM', '%b %d %Y %I:%M%p')\n",
    "\n",
    "predictions=pd.DataFrame()\n",
    "for i in range(104, len(tests)):\n",
    "    print(tests[i].split(\"/\")[5], end =\" \")\n",
    "    if (not useFlag):\n",
    "        headers=pd.read_csv(root+\"headers.csv\")\n",
    "        headers = headers.columns\n",
    "        test = pd.read_csv(tests[i], names=headers, header=None)\n",
    "        test['S_2'] = pd.to_datetime(test['S_2'])\n",
    "\n",
    "        dateshift = pd.DataFrame(test.groupby(['customer_ID'])['S_2'].agg(['min', 'count'])).rename(columns={\"min\": \"S_2\"}).reset_index()\n",
    "        dateshift['S_2'] = pd.to_datetime(dateshift['S_2'])\n",
    "        dateshift['mondiff'] = ((dateshift['S_2'].dt.year * 12) + dateshift['S_2'].dt.month) - ((mindate.year*12) + mindate.month)\n",
    "\n",
    "        test = pd.merge(test, dateshift[['customer_ID', 'mondiff']], on=['customer_ID'], how='inner')\n",
    "        test['S_2'] = test.apply(lambda x: x['S_2'] - pd.offsets.DateOffset(months=x['mondiff']), axis=1)\n",
    "        test['yearmonth'] = test['S_2'].dt.strftime(\"%Y%m\")\n",
    "\n",
    "        test=pd.merge(test, pd.merge(pd.DataFrame(test['customer_ID'].unique(), columns=['customer_ID']), dates, how='cross'), on=['customer_ID', 'yearmonth'], how='outer')\n",
    "        test['S_2'] = test.apply(lambda x: x['S_2_dt'] if pd.isnull(x['S_2']) else x['S_2'], axis=1)\n",
    "\n",
    "        test['day_of_month'] = test['S_2'].dt.strftime(\"%d\")\n",
    "        test['day_of_week'] = test['S_2'].dt.strftime(\"%u\")\n",
    "        test['day_of_year'] = test['S_2'].dt.strftime(\"%j\")\n",
    "        test.drop(columns=['S_2_dt'], inplace=True)\n",
    "        test.drop(columns=['mondiff'], inplace=True)\n",
    "\n",
    "        test = test.set_index(['customer_ID', 'yearmonth']).unstack(1).sort_index(axis=1, level=1)\n",
    "        test = test.reset_index()\n",
    "        test = pd.merge(test, dateshift[['customer_ID', 'count']], on=['customer_ID'], how='inner')\n",
    "\n",
    "        test = test.merge(labels, on=\"customer_ID\", how=\"left\")\n",
    "        test.drop(columns=[test.columns.to_list()[1]], inplace=True)\n",
    "\n",
    "        coldict={}\n",
    "        for col in test.columns:\n",
    "            coldict[col] = ''.join(col)\n",
    "\n",
    "        test.rename(coldict, axis=1, inplace=True)\n",
    "        test['target'] = 0\n",
    "\n",
    "#        test.to_csv(tests[i]+\".csv\", index=False, header=False)\n",
    "    else:\n",
    "        headers=pd.read_csv(root+\"/trains/headers.csv\")\n",
    "        headers = headers.columns\n",
    "        test = pd.read_csv(tests[i]+\".csv\", names=headers, header=None)\n",
    "\n",
    "    for col in [col for col in test.columns if (col[:len(col)-6] in ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']) or ('yearmonth' in col) or ('day_of_week' in col)]:\n",
    "        test[col] = test[col].astype(str)\n",
    "        test[col] = test[col].astype('category')\n",
    "\n",
    "    test['customer_ID2'] = test['customer_ID']\n",
    "    test.drop(columns=['customer_ID'], inplace=True)\n",
    "    test.rename(columns = {\"customer_ID2\": \"customer_ID\"}, inplace=True)\n",
    "    try:\n",
    "        preds = predict_model(dtmodels[len(dtmodels)-1], data=test, verbose=True)\n",
    "    except:\n",
    "        preds = predict_model(dtmodels[0], data=test, verbose=True)\n",
    "    predictions = pd.concat([predictions, preds], ignore_index=True, axis=0)\n",
    "#    preds[['customer_ID', 'Label']].to_csv(tests[i]+\".\" + \"\".join(algos) + s\n",
    "    preds[['customer_ID', 'Label']].to_csv(tests[i]+\".dt2\" + \".preds\", header=False, index=False)\n",
    "\n",
    "predictions.to_csv(\"test.preds\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d33efa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "mindate = datetime.strptime('Apr 1 2018  1:33PM', '%b %d %Y %I:%M%p')\n",
    "\n",
    "test = pd.read_csv(tests[i], names=headers, header=None)\n",
    "for col in [col for col in test.columns if col[:len(col)-6] in ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']]:\n",
    "    test[col] = test[col].astype(str)\n",
    "    test[col] = test[col].astype('category')\n",
    "\n",
    "test['S_2'] = pd.to_datetime(test['S_2'])\n",
    "\n",
    "dateshift = pd.DataFrame(test.groupby(['customer_ID'])['S_2'].min()).reset_index()\n",
    "dateshift['S_2'] = pd.to_datetime(dateshift['S_2'])\n",
    "dateshift['mondiff'] = ((dateshift['S_2'].dt.year * 12) + dateshift['S_2'].dt.month) - ((mindate.year*12) + mindate.month)\n",
    "\n",
    "test = pd.merge(test, dateshift[['customer_ID', 'mondiff']], on=['customer_ID'], how='inner')\n",
    "test['S_2'] = test.apply(lambda x: x['S_2'] - pd.offsets.DateOffset(months=x['mondiff']), axis=1)\n",
    "test['yearmonth'] = test['S_2'].dt.strftime(\"%Y%m\")\n",
    "\n",
    "test=pd.merge(test, pd.merge(pd.DataFrame(test['customer_ID'].unique(), columns=['customer_ID']), dates, how='cross'), on=['customer_ID', 'yearmonth'], how='outer')\n",
    "test['S_2'] = test.apply(lambda x: x['S_2_dt'] if pd.isnull(x['S_2']) else x['S_2'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dafe5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print((test[test['customer_ID']=='0a31ea77ca60c94e754c884616778043f32f445ad2765f9564567030b425c185']['S_2'][0].year*12 + test[test['customer_ID']=='0a31ea77ca60c94e754c884616778043f32f445ad2765f9564567030b425c185']['S_2'][0].month) - (mindate.year*12+mindate.month), (test[test['customer_ID']=='0a31ea77ca60c94e754c884616778043f32f445ad2765f9564567030b425c185']['S_2'][0].year*100 + test[test['customer_ID']=='0a31ea77ca60c94e754c884616778043f32f445ad2765f9564567030b425c185']['S_2'][0].month), (mindate.year*100+mindate.month))\n",
    "test\n",
    "test[test['customer_ID']=='0a31ea77ca60c94e754c884616778043f32f445ad2765f9564567030b425c185']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170965cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def testcolfillna(col):\n",
    "#    print(col, end=\"\")\n",
    "    try:\n",
    "        test[col] = test[col[:len(col)-6]+str(int(col[len(col)-6:len(col)-2])+1) + str(col[len(col)-2:len(col)])]\n",
    "    except:\n",
    "        print(col)\n",
    "    return\n",
    "\n",
    "def testfillna(col):\n",
    "#    print(\".\", end=\"\")\n",
    "    test[col].fillna(test[[column for column in test.columns.tolist() if column[:len(column)-3] in col]].mean(axis=1), inplace=True)\n",
    "\n",
    "#    with Pool(12) as pool:\n",
    "#        pool.map(testcolfillna, [col for col in test.columns[test.isna().all()]])\n",
    "\n",
    "#    with Pool(12) as pool:\n",
    "#        pool.map(testfillna, [col for col in test.columns[test.isna().any()]])\n",
    "\n",
    "#    for col in test.columns[test.isna().all()]:\n",
    "#        test[col] = test[col[:len(col)-6]+str(int(col[len(col)-6:len(col)-2])+1) + str(col[len(col)-2:len(col)])]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0ced3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf = pd.read_csv(root+\"/x12-0.csv\")\n",
    "traindf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8908c0de-5489-4c22-8e17-fddad200a3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(root+\"/x3dtcnt.csv\")\n",
    "df[df['yearmonthorig201903'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0e1020-da9a-4d14-a3e7-f1fd24e67e27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
