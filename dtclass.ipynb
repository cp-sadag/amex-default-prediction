{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f457426d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pycaret.regression import *\n",
    "#from multiprocessing import Pool\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from operator import attrgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a090788f-2ba4-45bf-aa8d-09c6ef22d2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "root=\"/home/sadagopan/amex-default-prediction/\"\n",
    "headers=pd.read_csv(root+\"headers.csv\")\n",
    "headers = headers.columns\n",
    "scols = [col for col in headers if col[0] == 'S']\n",
    "dcols = [col for col in headers if col[0] == 'D']\n",
    "rcols = [col for col in headers if col[0] == 'R']\n",
    "bcols = [col for col in headers if col[0] == 'B']\n",
    "pcols = [col for col in headers if col[0] == 'P']\n",
    "labels =pd.read_csv(root+\"train_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e024910",
   "metadata": {},
   "outputs": [],
   "source": [
    "trains = []\n",
    "for r, d, f in os.walk(root):\n",
    "    f.sort()\n",
    "    for file in f:\n",
    "#        if \"traina\" in file or \"trainb\" in file:\n",
    "        if 'pkl' not in file and 'csv' not in file and (\"xa\" in file or \"xb\" in file or file.startswith(\"xc\")):\n",
    "            trains.append(os.path.join(root, file))\n",
    "\n",
    "dates=pd.DataFrame(pd.date_range('2018-04-01','2019-04-30', freq='MS').tolist(), columns=['S_2_dt'])\n",
    "dates['yearmonth'] = dates['S_2_dt'].dt.strftime(\"%Y%m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a510522-8c10-426c-86f0-1546aaf99b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#traindf = pd.concat(traindfs, ignore_index=True)\n",
    "headers=pd.read_csv(root+\"headers.csv\")\n",
    "headers = headers.columns\n",
    "\n",
    "traindf = pd.read_csv(root+\"/tdata\")\n",
    "tdf = traindf.groupby('customer_ID')['S_2'].agg(['min', 'max', 'count']).reset_index()\n",
    "#tdf = tdf.merge(labels, on=\"customer_ID\", how=\"inner\")\n",
    "#for i in range(13):\n",
    "#    print(i, tdf[tdf['S_2']==i].count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d1243a9-4751-474f-8a25-f81257abb215",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf.rename(columns={\"min\": \"mindate\", \"max\": \"maxdate\"}, inplace=True)\n",
    "\n",
    "tdf['maxdate'] = pd.to_datetime(tdf['maxdate'])\n",
    "tdf['mindate'] = pd.to_datetime(tdf['mindate'])\n",
    "\n",
    "#tdf['diffcount'] = tdf.apply(lambda x: ((x['maxdate'] - x['mindate'])/np.timedelta64(1, 'M')))\n",
    "\n",
    "tdf['mondiff'] = (tdf.maxdate.dt.strftime(\"%Y%m\").astype(int) - tdf.mindate.dt.strftime(\"%Y%m\").astype(int) - 87)\n",
    "tdf['mondiff'] = tdf['mondiff'].apply(lambda x: x+88 if x< 0 else x)\n",
    "#tdf[tdf['count'] == tdf['mondiff']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e21005-cdab-4347-84a7-b6e8c80e7654",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindfs = []\n",
    "\n",
    "for k in range(12):\n",
    "    traindfs.append(pd.DataFrame(columns=headers))\n",
    "k=0\n",
    "for i in range(len(trains)):\n",
    "    print(trains[i])\n",
    "    traindf = pd.read_csv(trains[i], names=headers, header=None)\n",
    "    for j in range(1, 13):\n",
    "        mask = traindf.customer_ID.isin(tdf[tdf['mondiff'] == j].customer_ID)\n",
    "        traindfs[j-1] = pd.concat([traindfs[j-1], traindf[mask]], ignore_index=True)\n",
    "        if traindfs[j-1]['customer_ID'].count() >= 260000:\n",
    "            traindfs[j-1] = traindfs[j-1].merge(labels, on=\"customer_ID\", how=\"inner\")\n",
    "            traindfs[j-1].iloc[:260000,].to_csv(\"x\" + str(j-1) + \"-\" + str(k) + \".csv\", index=False)\n",
    "            traindfs[j-1] = traindfs[j-1].iloc[260000:,]\n",
    "            traindfs[j-1].drop(columns=['target'], inplace=True)\n",
    "            k+=1            \n",
    "\n",
    "for i in range(len(traindfs)):\n",
    "    traindfs[i] = traindfs[i].merge(labels, on=\"customer_ID\", how=\"inner\")\n",
    "    traindfs[i].to_csv(\"x\" + str(i) + \".csv\", index=False)\n",
    "\n",
    "#for i in range(12, len(traindfs)):\n",
    "#    traindfs[i] = traindfs[i].merge(labels, on=\"customer_ID\", how=\"inner\")\n",
    "#    traindfs[i].to_csv(\"x\" + str(i) + \"-\" + str(k) + \".csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926a11b1-28b3-45d5-bbc8-b4b1a58ee9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindfs = []\n",
    "i=0\n",
    "for j in range(12):\n",
    "    print(\"x\" + str(j) + \".csv\")\n",
    "    traindfs.append(pd.read_csv(\"x\" + str(j) + \".csv\"))\n",
    "\n",
    "    mindate = datetime.strptime('Apr 1 2018 1:33PM', '%b %d %Y %I:%M%p')\n",
    "    maxdate = mindate + pd.offsets.DateOffset(months=12)\n",
    "    dates=pd.DataFrame(pd.date_range(mindate, maxdate, freq='MS').tolist(), columns=['S_2_dt'])\n",
    "    dates['yearmonth'] = dates['S_2_dt'].dt.strftime(\"%Y%m\")\n",
    "    dates.reset_index(inplace=True)\n",
    "    dates.rename(columns={'index': 'seq'}, inplace=True)\n",
    "    dates['seq']=(dates['seq']+1)\n",
    "\n",
    "    traindfs[i]['S_2'] = pd.to_datetime(traindfs[i]['S_2'])\n",
    "    traindfs[i]['S_2'] = traindfs[i]['S_2'] + pd.offsets.DateOffset(years=1)\n",
    "    traindfs[i]['S_2'] = traindfs[i]['S_2'] + pd.offsets.DateOffset(months=1)\n",
    "    traindfs[i]['yearmonth'] = traindfs[i]['S_2'].dt.strftime(\"%Y%m\")\n",
    "    traindfs[i]['seq'] = traindfs[i].groupby(['customer_ID']).cumcount().add(1)\n",
    "    traindfs[i][['sncnt', 'dncnt', 'rncnt', 'bncnt', 'pncnt']] = traindfs[i].apply(lambda x: (sum([(2^scols.index(col)) for col in scols if pd.isnull(x[col])]),\n",
    "                sum([(2^dcols.index(col)) for col in dcols if pd.isnull(x[col])]),\n",
    "                sum([(2^rcols.index(col)) for col in rcols if pd.isnull(x[col])]),\n",
    "                sum([(2^bcols.index(col)) for col in bcols if pd.isnull(x[col])]),\n",
    "                sum([(2^pcols.index(col)) for col in pcols if pd.isnull(x[col])])), axis=1, result_type='expand')\n",
    "\n",
    "    dateshift = pd.DataFrame(traindfs[i].groupby(['customer_ID'])['S_2'].agg(['min', 'count'])).rename(columns={\"min\": \"S_2\"}).reset_index()\n",
    "    dateshift['S_2'] = pd.to_datetime(dateshift['S_2'])\n",
    "    dateshift['mondiff'] = dateshift['S_2'].dt.month - 4\n",
    "    dateshift['mondiff'] = dateshift['mondiff'].apply(lambda x: 12+x if x <= 0 else x)\n",
    "\n",
    "    traindfs[i] = pd.merge(traindfs[i], dateshift[['customer_ID', 'mondiff']], on=['customer_ID'], how='inner')\n",
    "    traindfs[i]['S_2'] = traindfs[i].apply(lambda x: x['S_2'] - pd.offsets.DateOffset(months=x['mondiff']), axis=1)\n",
    "\n",
    "    traindfs[i]=pd.merge(traindfs[i], pd.merge(pd.DataFrame(traindfs[i]['customer_ID'].unique(), columns=['customer_ID']), dates, how='cross'), left_on=['customer_ID', 'seq'], right_on=['customer_ID', 'seq'], how='outer')\n",
    "#    traindfs[i]['yearmonth'] = traindfs[i].apply(lambda x: x['yearmonth_y'] if pd.isnull(x['yearmonth_x']) else x['yearmonth['S_2_'], axis=1)\n",
    "    traindfs[i]['origmissrcnt'] = traindfs[i].apply(lambda x: (2**((((x['S_2_dt'].year-2018) * 12 ) + x['S_2_dt'].month) - 4)) if (pd.isnull(x['S_2']) and x['seq'] <=j) else 0, axis=1)\n",
    "    traindfs[i]['totmissrcnt'] = traindfs[i].apply(lambda x: (2**((((x['S_2_dt'].year-2018) * 12 ) + x['S_2_dt'].month) - 4)) if (pd.isnull(x['S_2'])) else 0, axis=1)\n",
    "    dateshift = pd.merge(dateshift, pd.DataFrame(traindfs[i].groupby(['customer_ID'])['origmissrcnt'].sum()).reset_index(), on ='customer_ID', how='inner')\n",
    "    dateshift = pd.merge(dateshift, pd.DataFrame(traindfs[i].groupby(['customer_ID'])['totmissrcnt'].sum()).reset_index(), on ='customer_ID', how='inner')\n",
    "    traindfs[i]['S_2'] = traindfs[i].apply(lambda x: x['S_2_dt'] if (pd.isnull(x['S_2']) or x['S_2'].month != x['S_2_dt'].month) else x['S_2'], axis=1)\n",
    "    traindfs[i].drop(columns=['S_2_dt', 'origmissrcnt', 'totmissrcnt', 'seq'], inplace=True)\n",
    "    traindfs[i] = traindfs[i].rename(columns={\"yearmonth_x\": \"yearmonth\"})\n",
    "\n",
    "    traindfs[i]['mondiff'].fillna(0, inplace=True)\n",
    "    traindfs[i]['day_of_month'] = traindfs[i]['S_2'].dt.strftime(\"%d\")\n",
    "    traindfs[i]['day_of_week'] = traindfs[i]['S_2'].dt.strftime(\"%u\")\n",
    "    traindfs[i]['day_of_year'] = traindfs[i]['S_2'].dt.strftime(\"%j\")\n",
    "    traindfs[i]['week_of_year'] = traindfs[i]['S_2'].dt.isocalendar().week\n",
    "    traindfs[i].drop(columns=['mondiff'], inplace=True)\n",
    "    traindfs[i].drop(columns=['S_2'], inplace=True)\n",
    "    traindfs[i].drop(columns=['target'], inplace=True)\n",
    "\n",
    "#    df = traindfs[i].groupby(\"day_of_year\").transform(lambda x: x.fillna(x.mean()))\n",
    "#    for col in traindfs[i][traindfs[i].select_dtypes(include=['int64', 'float64']).columns].isna().any().index:\n",
    "#        traindfs[i][col] = df[col]\n",
    "    \n",
    "#    df = traindfs[i].groupby(\"week_of_year\").transform(lambda x: x.fillna(x.mean()))\n",
    "#    for col in traindfs[i][traindfs[i].select_dtypes(include=['int64', 'float64']).columns].isna().any().index:\n",
    "#        traindfs[i][col] = df[col]\n",
    "\n",
    "#    df = traindfs[i].groupby(\"day_of_month\").transform(lambda x: x.fillna(x.mean()))\n",
    "#    for col in traindfs[i][traindfs[i].select_dtypes(include=['int64', 'float64']).columns].isna().any().index:\n",
    "#        traindfs[i][col] = df[col]\n",
    "\n",
    "#    df = traindfs[i].groupby(\"yearmonth\").transform(lambda x: x.fillna(x.mean()))\n",
    "#    for col in traindfs[i][traindfs[i].select_dtypes(include=['int64', 'float64']).columns].isna().any().index:\n",
    "#        traindfs[i][col] = df[col]\n",
    "\n",
    "#    df = traindfs[i].groupby(\"day_of_week\").transform(lambda x: x.fillna(x.mean()))\n",
    "#    for col in traindfs[i][traindfs[i].select_dtypes(include=['int64', 'float64']).columns].isna().any().index:\n",
    "#        traindfs[i][col] = df[col]\n",
    "\n",
    "    traindfs[i]=traindfs[i].set_index(['customer_ID', 'yearmonth_y']).unstack(1).sort_index(axis=1, level=1)\n",
    "    traindfs[i] = traindfs[i].reset_index()\n",
    "    traindfs[i] = pd.merge(traindfs[i], dateshift[['customer_ID', 'origmissrcnt', 'totmissrcnt']], on=['customer_ID'], how='inner')\n",
    "\n",
    "    cols=traindfs[i].columns\n",
    "    for col in traindfs[i][traindfs[i].select_dtypes(include=['int64', 'float64']).columns].isna().any().index:\n",
    "        traindfs[i][col].fillna(traindfs[i][[column for column in cols if col[:len(col)-3] == column[:len(column)-3]]].mean(axis=1), inplace=True)\n",
    "\n",
    "    for col in traindfs[i][traindfs[i].select_dtypes(include=['int64', 'float64']).columns].isna().any().index:\n",
    "        traindfs[i][col].fillna(traindfs[i][col].mean(), inplace=True)\n",
    "\n",
    "    traindfs[i] = traindfs[i].merge(labels, on=\"customer_ID\", how=\"inner\")\n",
    "    traindfs[i].drop(columns=[traindfs[i].columns.to_list()[1]], inplace=True)\n",
    "\n",
    "    coldict={}\n",
    "    for col in traindfs[i].columns:\n",
    "        coldict[col] = ''.join(col)\n",
    "    traindfs[i].rename(coldict, axis=1, inplace=True)\n",
    "\n",
    "#    for column in [col for col in traindfs[0].columns if col[len(col)-6:] in ['201904', '201903']]:\n",
    "#        traindfs[0].drop(columns=[column], inplace=True)\n",
    "\n",
    "    traindfs[i].to_csv(\"x\" + str(j) +\"dtcnt.csv\", index=False)\n",
    "    i+=1\n",
    "\n",
    "traindfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de7710d-861f-40f1-bb38-af9b67f02554",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindfs = []\n",
    "i=0\n",
    "j=12\n",
    "for k in range(19):\n",
    "    print(\"x\" + str(j) + \"-\" + str(k) + \".csv\")\n",
    "    traindfs.append(pd.read_csv(\"x\" + str(j) + \"-\" + str(k) + \".csv\"))\n",
    "\n",
    "    mindate = datetime.strptime('Apr 1 2018  1:33PM', '%b %d %Y %I:%M%p')\n",
    "    maxdate = mindate + pd.offsets.DateOffset(months=j)\n",
    "    dates=pd.DataFrame(pd.date_range(mindate, maxdate, freq='MS').tolist(), columns=['S_2_dt'])\n",
    "    dates['yearmonth'] = dates['S_2_dt'].dt.strftime(\"%Y%m\")\n",
    "    dates.reset_index(inplace=True)\n",
    "    dates.rename(columns={'index': 'seq'}, inplace=True)\n",
    "    dates['seq']=dates['seq']+1\n",
    "\n",
    "    traindfs[i]['S_2'] = pd.to_datetime(traindfs[i]['S_2'])\n",
    "    traindfs[i]['S_2'] = traindfs[i]['S_2'] + pd.offsets.DateOffset(years=1)\n",
    "    traindfs[i]['S_2'] = traindfs[i]['S_2'] + pd.offsets.DateOffset(months=1)\n",
    "    traindfs[i]['yearmonth'] = traindfs[i]['S_2'].dt.strftime(\"%Y%m\")\n",
    "    traindfs[i]['seq'] = traindfs[i].groupby(['customer_ID']).cumcount().add(1)\n",
    "    traindfs[i][['sncnt', 'dncnt', 'rncnt', 'bncnt', 'pncnt']] = traindfs[i].apply(lambda x: (sum([(2^scols.index(col)) for col in scols if pd.isnull(x[col])]),\n",
    "                         sum([(2^dcols.index(col)) for col in dcols if pd.isnull(x[col])]),\n",
    "                         sum([(2^rcols.index(col)) for col in rcols if pd.isnull(x[col])]),\n",
    "                         sum([(2^bcols.index(col)) for col in bcols if pd.isnull(x[col])]),\n",
    "                         sum([(2^pcols.index(col)) for col in pcols if pd.isnull(x[col])])), axis=1, result_type='expand')\n",
    "\n",
    "    dateshift = pd.DataFrame(traindfs[i].groupby(['customer_ID'])['S_2'].agg(['min', 'count'])).rename(columns={\"min\": \"S_2\"}).reset_index()\n",
    "    dateshift['S_2'] = pd.to_datetime(dateshift['S_2'])\n",
    "    dateshift['mondiff'] = dateshift['S_2'].dt.month - 4\n",
    "    dateshift['mondiff'] = dateshift['mondiff'].apply(lambda x: 12+x if x <0 else x)\n",
    "\n",
    "    traindfs[i] = pd.merge(traindfs[i], dateshift[['customer_ID', 'mondiff']], on=['customer_ID'], how='inner')\n",
    "    traindfs[i]['S_2'] = traindfs[i].apply(lambda x: x['S_2'] - pd.offsets.DateOffset(months=x['mondiff']), axis=1)\n",
    "    traindfs[i]['yearmonthnew'] = traindfs[i]['S_2'].dt.strftime(\"%Y%m\")\n",
    "\n",
    "    traindfs[i]=pd.merge(traindfs[i], pd.merge(pd.DataFrame(traindfs[i]['customer_ID'].unique(), columns=['customer_ID']), dates, how='cross'), on=['customer_ID', 'seq'], how='inner')\n",
    "    traindfs[i]['S_2'] = traindfs[i].apply(lambda x: x['S_2_dt'] if pd.isnull(x['S_2']) else x['S_2'], axis=1)\n",
    "    traindfs[i].drop(columns=['S_2_dt', 'yearmonth_y'], inplace=True)\n",
    "    traindfs[i] = traindfs[i].rename(columns={\"yearmonth_x\": \"yearmonth\"})\n",
    "\n",
    "    traindfs[i]['mondiff'].fillna(0, inplace=True)\n",
    "    traindfs[i]['day_of_month'] = traindfs[i]['S_2'].dt.strftime(\"%d\")\n",
    "    traindfs[i]['day_of_week'] = traindfs[i]['S_2'].dt.strftime(\"%u\")\n",
    "    traindfs[i]['day_of_year'] = traindfs[i]['S_2'].dt.strftime(\"%j\")\n",
    "    traindfs[i].drop(columns=['mondiff'], inplace=True)\n",
    "    traindfs[i].drop(columns=['S_2'], inplace=True)\n",
    "    traindfs[i].drop(columns=['target'], inplace=True)\n",
    "\n",
    "    traindfs[i]=traindfs[i].set_index(['customer_ID', 'yearmonthnew']).unstack(1).sort_index(axis=1, level=1)\n",
    "    traindfs[i] = traindfs[i].reset_index()\n",
    "    traindfs[i] = pd.merge(traindfs[i], dateshift[['customer_ID', 'count']], on=['customer_ID'], how='inner')\n",
    "\n",
    "    for col in traindfs[i][traindfs[i].select_dtypes(include=['int64', 'float64']).columns].isna().any().index:\n",
    "        traindfs[i][col].fillna(traindfs[i][[column for column in traindfs[i].columns.tolist() if col[:len(col)-3] == column[:len(column)-3]]].mean(axis=1), inplace=True)\n",
    "\n",
    "    for col in traindfs[i][traindfs[i].select_dtypes(include=['int64', 'float64']).columns].isna().any().index:\n",
    "        traindfs[i][col].fillna(traindfs[i][col].mean(), inplace=True)\n",
    "\n",
    "    traindfs[i] = traindfs[i].merge(labels, on=\"customer_ID\", how=\"inner\")\n",
    "    traindfs[i].drop(columns=[traindfs[i].columns.to_list()[1]], inplace=True)\n",
    "\n",
    "    coldict={}\n",
    "    for col in traindfs[i].columns:\n",
    "        coldict[col] = ''.join(col)\n",
    "    traindfs[i].rename(coldict, axis=1, inplace=True)\n",
    "\n",
    "    for column in [col for col in traindfs[0].columns if col[len(col)-6:] in ['201904', '201903']]:\n",
    "        traindfs[0].drop(columns=[column], inplace=True)\n",
    "\n",
    "    traindfs[i].to_csv(\"x\" + str(j) + \"-\" + str(k) +\"dtcnt.csv\", index=False)\n",
    "    i+=1\n",
    "\n",
    "traindfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef42934",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "traindfs = []\n",
    "\n",
    "for j in range(len(trains)):\n",
    "    i = j#-18\n",
    "    print(trains[j])\n",
    "    traindfs.append(pd.read_csv(trains[j], names=headers, header=None))\n",
    "\n",
    "    traindfs[i]['S_2'] = pd.to_datetime(traindfs[i]['S_2'])\n",
    "    traindfs[i]['S_2'] = traindfs[i]['S_2'] + pd.offsets.DateOffset(years=1)\n",
    "    traindfs[i]['S_2'] = traindfs[i]['S_2'] + pd.offsets.DateOffset(months=1)\n",
    "    traindfs[i]['yearmonth'] = traindfs[i]['S_2'].dt.strftime(\"%Y%m\")\n",
    "\n",
    "    dateshift = pd.DataFrame(traindfs[i].groupby(['customer_ID'])['S_2'].agg(['min', 'count'])).rename(columns={\"min\": \"S_2\"}).reset_index()\n",
    "    dateshift['S_2'] = pd.to_datetime(dateshift['S_2'])\n",
    "    dateshift['mondiff'] = dateshift['S_2'].dt.month - 4\n",
    "    dateshift['mondiff'] = dateshift['mondiff'].apply(lambda x: 12+x if x <0 else x)\n",
    "\n",
    "    traindfs[i] = pd.merge(traindfs[i], dateshift[['customer_ID', 'mondiff']], on=['customer_ID'], how='inner')\n",
    "    traindfs[i]['S_2'] = traindfs[i].apply(lambda x: x['S_2'] - pd.offsets.DateOffset(months=x['mondiff']), axis=1)\n",
    "\n",
    "    traindfs[i]=pd.merge(traindfs[i], pd.merge(pd.DataFrame(traindfs[i]['customer_ID'].unique(), columns=['customer_ID']), dates, how='cross'), on=['customer_ID', 'yearmonth'], how='outer')\n",
    "    traindfs[i]['S_2'] = traindfs[i].apply(lambda x: x['S_2_dt'] if pd.isnull(x['S_2']) else x['S_2'], axis=1)\n",
    "    traindfs[i].drop(columns=['S_2_dt'], inplace=True)\n",
    "\n",
    "    traindfs[i]['mondiff'].fillna(0, inplace=True)\n",
    "#    for col in traindfs[i][traindfs[i].select_dtypes(include=['int64', 'float64']).columns].isna().any().index:\n",
    "#        traindfs[i][col] = traindfs[i].groupby('customer_ID')[col].transform(lambda grp: grp.fillna(np.mean(grp)))\n",
    "#    for col in traindfs[i].columns[traindfs[i].isna().any()]:\n",
    "#        if traindfs[i][col].dtype in ['float64', 'int64']:\n",
    "#            traindfs[i][col].fillna(traindfs[i][col].mean(), inplace=True)\n",
    "    traindfs[i]['day_of_month'] = traindfs[i]['S_2'].dt.strftime(\"%d\")\n",
    "    traindfs[i]['day_of_week'] = traindfs[i]['S_2'].dt.strftime(\"%u\")\n",
    "    traindfs[i]['day_of_year'] = traindfs[i]['S_2'].dt.strftime(\"%j\")\n",
    "    traindfs[i].drop(columns=['mondiff'], inplace=True)\n",
    "    traindfs[i].drop(columns=['S_2'], inplace=True)\n",
    "\n",
    "    traindfs[i]=traindfs[i].set_index(['customer_ID', 'yearmonth']).unstack(1).sort_index(axis=1, level=1)\n",
    "    traindfs[i] = traindfs[i].reset_index()\n",
    "    traindfs[i] = pd.merge(traindfs[i], dateshift[['customer_ID', 'count']], on=['customer_ID'], how='inner')\n",
    "\n",
    "#    for col in traindfs[i][traindfs[i].select_dtypes(include=['int64', 'float64']).columns].isna().any().index:\n",
    "#        traindfs[i][col].fillna(traindfs[i][[column for column in traindfs[i].columns.tolist() if col[:len(col)-3] == column[:len(column)-3]]].mean(axis=1), inplace=True)\n",
    "\n",
    "#    for col in traindfs[i][traindfs[i].select_dtypes(include=['int64', 'float64']).columns].isna().any().index:\n",
    "#        traindfs[i][col].fillna(traindfs[i][col].mean(), inplace=True)\n",
    "\n",
    "    traindfs[i] = traindfs[i].merge(labels, on=\"customer_ID\", how=\"inner\")\n",
    "    traindfs[i].drop(columns=[traindfs[i].columns.to_list()[1]], inplace=True)\n",
    "\n",
    "    coldict={}\n",
    "    for col in traindfs[i].columns:\n",
    "        coldict[col] = ''.join(col)\n",
    "    traindfs[i].rename(coldict, axis=1, inplace=True)\n",
    "\n",
    "    traindfs[i].to_csv(trains[i]+\"nona.csv\", index=False, header=False)\n",
    "\n",
    "traindfs[0][traindfs[0]['customer_ID']==0].to_csv(root+\"/trains/headers.csv\", index=False)\n",
    "traindfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0e8567-2a12-4166-8446-5b48e2041ba0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34edcfb-4b4e-4f66-81e1-60eea9fd3084",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#traindf = pd.concat(traindfs, ignore_index=True)\n",
    "headers=pd.read_csv(root+\"headers.csv\")\n",
    "headers = headers.columns\n",
    "\n",
    "traindf = pd.read_csv(root+\"/tdata\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa3f62a-6ddf-438a-b79e-7d3a82e9a1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#traindf = traindf[traindf['target']==1]\n",
    "#for col in [col for col in traindf.columns if (col[len(col)-6:]=='201804')]:\n",
    "#    traindf = traindf[~traindf[col].isnull()]\n",
    "\n",
    "#traindf=traindf[~traindf['B_12201804'].isnull()]\n",
    "#traindf['B_42201804'].unique()\n",
    "\n",
    "#(traindf.groupby('customer_ID')['S_2'].agg([\"min\", \"max\"]).reset_index())['min'].max(), (traindf.groupby('customer_ID')['S_2'].agg([\"min\", \"max\"]).reset_index())['max'].min()\n",
    "\n",
    "tdf = traindf.groupby('customer_ID')['S_2'].count().reset_index()\n",
    "tdf = tdf.merge(labels, on=\"customer_ID\", how=\"inner\")\n",
    "for i in range(1, 13):\n",
    "    print(i, tdf[tdf['S_2']==i].count())\n",
    "\n",
    "#traindf.groupby('customer_ID')['S_2'].count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659ec1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers=pd.read_csv(root+\"/trains/headers.csv\")\n",
    "headers = headers.columns\n",
    "\n",
    "traindf=pd.DataFrame()\n",
    "\n",
    "for i in range(12):\n",
    "    traindf = pd.concat([traindf, pd.read_csv(trains[i]+\"new.csv\", names=headers, header=None)], ignore_index=True)\n",
    "print(\"loaded data...\")\n",
    "\n",
    "for col in [col for col in traindf.columns if (col[:len(col)-6] in ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']) or ('yearmonth' in col) or ('day_of_week' in col)]:\n",
    "    traindf[col] = traindf[col].astype(str)\n",
    "    traindf[col] = traindf[col].astype('category')\n",
    "print(\"updated categories...\")\n",
    "\n",
    "traindf.groupby(['target'])['target'].count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a094cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = setup(data=traindf, target='target',\n",
    "          transformation=True,\n",
    "          ignore_features=['customer_ID'],\n",
    "          categorical_features = [col for col in traindf.columns if (col[:len(col)-6] in ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']) or ('yearmonth' in col) or ('day_of_week' in col)],\n",
    "          numeric_features=[col for col in traindf.columns if ('day_of_month') in col or ('day_of_year' in col)],\n",
    "#          normalize = True,\n",
    "          remove_outliers=True, handle_unknown_categorical=True,fix_imbalance=True,\n",
    "          combine_rare_levels=True, create_clusters=True,\n",
    "          feature_selection=True, remove_multicollinearity=True, pca=True, ignore_low_variance=True,\n",
    "          train_size=0.8, fold=6,\n",
    "          use_gpu=True,\n",
    "          silent = True, session_id = 123, verbose=True)\n",
    "save_config(\"/amex/dt3regconfig.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cf08b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#eda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48224110",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load_config(\"/amex/dtclassconfig.pkl\")\n",
    "dt = create_model('dt', fold=6, verbose=True)\n",
    "print(\"created\")\n",
    "save_model(dt, \"/amex/models/dt2reg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e096133-e021-4221-acc5-1ed6b5d7b65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(dt, \"/amex/models/dt1Lclass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e522747b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = tune_model(dt, fold=6, n_iter=10, choose_better=True)\n",
    "print(\"tuned\")\n",
    "save_model(dt, \"e:/rebid/amex/models/dt1Lclasstuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0440bf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = finalize_model(dt)\n",
    "print(\"finalized\")\n",
    "#optimize_threshold(dt)\n",
    "#print(\"optimized\")\n",
    "save_model(dt, \"e:/rebid/amex/models/dt1Lclassfin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3124d641",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibrated_dt = calibrate_model(dt)\n",
    "save_model(calibrated_dt, \"/amex/models/dt1Lclasscalib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ade2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(dt, plot='calibration')\n",
    "plot_model(model, plot='calibration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea60f6b-4505-4ccf-8c90-2e9dfd02477f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dtmodels=[]\n",
    "dtsets=[]\n",
    "headers=pd.read_csv(root+\"/trains/headers.csv\")\n",
    "headers = headers.columns\n",
    "n=6\n",
    "algos=['dt', 'catboost', 'lightgbm']\n",
    "loadconfig=True\n",
    "loadmodels=[False, False, False]\n",
    "\n",
    "for i in range(1):\n",
    "    if loadconfig:\n",
    "        dtsets.append(load_config(root+\"/configs/\" + 'dt' + str(n) + \"-\" + str(i) + \"config.pkl\"))\n",
    "    else:\n",
    "        print(i, end=\"\")\n",
    "        traindf=pd.DataFrame()\n",
    "        for j in range(i*n, (i*n)+n):\n",
    "            print(\".\"+str(j), end=\"\")\n",
    "            traindf = pd.concat([traindf, pd.read_csv(trains[i]+\"new1.csv\", names=headers, header=None)], ignore_index=True)\n",
    "\n",
    "        for col in [col for col in traindf.columns if (col[:len(col)-6] in ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']) or ('yearmonth' in col) or ('day_of_week' in col)]:\n",
    "            traindf[col] = traindf[col].astype(str)\n",
    "            traindf[col] = traindf[col].astype('category')\n",
    "\n",
    "        dtsets.append(setup(data=traindf, target='target',\n",
    "              transformation=True,\n",
    "              ignore_features=['customer_ID'],\n",
    "              categorical_features = [col for col in traindf.columns if (col[:len(col)-6] in ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']) or ('yearmonth' in col) or ('day_of_week' in col)],\n",
    "              numeric_features=[col for col in traindf.columns if ('day_of_month') in col or ('day_of_year' in col)],\n",
    "              remove_outliers=True, handle_unknown_categorical=True,\n",
    "              combine_rare_levels=True, \n",
    "              create_clusters=True,\n",
    "#              transform_target=True, transform_target_method='yeo-johnson',\n",
    "              polynomial_features=True,\n",
    "              feature_selection=True, remove_multicollinearity=True, pca=True, ignore_low_variance=True,\n",
    "              train_size=0.8, fold=6,\n",
    "              use_gpu=True,\n",
    "              silent = True, session_id = 123, verbose=True))\n",
    "        save_config(root+\"/configs/\" + algo + str(n) + \"-\" + str(i) + \"config.pkl\")\n",
    "\n",
    "    for j in range(len(algos)):\n",
    "        if loadmodels[j]:\n",
    "            dtmodels.append(load_model(root+\"/models/\" + algos[j] + str(n)+\"-\"+str(i)+\"reg\"))\n",
    "        else:\n",
    "            dtmodels.append(create_model(algos[j], fold=6, verbose=True))\n",
    "            save_model(dtmodels[i], root+\"/models/\" + algos[j] + str(n)+\"-\"+str(i)+\"reg\")\n",
    "\n",
    "dtmodels.append(stack_models(dtmodels, meta_model = dtmodels[len(dtmodels)-1]))\n",
    "save_model(dtmodels[len(dtmodels)-1], root+\"/models/\" + \"\".join(algos) + \"stackedreg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f6f10de-0c23-4851-ae37-29423c7bd86c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_def24_row10_col0, #T_def24_row10_col1, #T_def24_row10_col2, #T_def24_row10_col3, #T_def24_row10_col4, #T_def24_row10_col5 {\n",
       "  background: yellow;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_def24\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_def24_level0_col0\" class=\"col_heading level0 col0\" >MAE</th>\n",
       "      <th id=\"T_def24_level0_col1\" class=\"col_heading level0 col1\" >MSE</th>\n",
       "      <th id=\"T_def24_level0_col2\" class=\"col_heading level0 col2\" >RMSE</th>\n",
       "      <th id=\"T_def24_level0_col3\" class=\"col_heading level0 col3\" >R2</th>\n",
       "      <th id=\"T_def24_level0_col4\" class=\"col_heading level0 col4\" >RMSLE</th>\n",
       "      <th id=\"T_def24_level0_col5\" class=\"col_heading level0 col5\" >MAPE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >Fold</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_def24_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_def24_row0_col0\" class=\"data row0 col0\" >0.2735</td>\n",
       "      <td id=\"T_def24_row0_col1\" class=\"data row0 col1\" >0.2735</td>\n",
       "      <td id=\"T_def24_row0_col2\" class=\"data row0 col2\" >0.5230</td>\n",
       "      <td id=\"T_def24_row0_col3\" class=\"data row0 col3\" >-0.1149</td>\n",
       "      <td id=\"T_def24_row0_col4\" class=\"data row0 col4\" >0.3625</td>\n",
       "      <td id=\"T_def24_row0_col5\" class=\"data row0 col5\" >0.3152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_def24_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_def24_row1_col0\" class=\"data row1 col0\" >0.2598</td>\n",
       "      <td id=\"T_def24_row1_col1\" class=\"data row1 col1\" >0.2598</td>\n",
       "      <td id=\"T_def24_row1_col2\" class=\"data row1 col2\" >0.5097</td>\n",
       "      <td id=\"T_def24_row1_col3\" class=\"data row1 col3\" >-0.0601</td>\n",
       "      <td id=\"T_def24_row1_col4\" class=\"data row1 col4\" >0.3533</td>\n",
       "      <td id=\"T_def24_row1_col5\" class=\"data row1 col5\" >0.3005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_def24_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_def24_row2_col0\" class=\"data row2 col0\" >0.2690</td>\n",
       "      <td id=\"T_def24_row2_col1\" class=\"data row2 col1\" >0.2690</td>\n",
       "      <td id=\"T_def24_row2_col2\" class=\"data row2 col2\" >0.5186</td>\n",
       "      <td id=\"T_def24_row2_col3\" class=\"data row2 col3\" >-0.1026</td>\n",
       "      <td id=\"T_def24_row2_col4\" class=\"data row2 col4\" >0.3595</td>\n",
       "      <td id=\"T_def24_row2_col5\" class=\"data row2 col5\" >0.3161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_def24_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_def24_row3_col0\" class=\"data row3 col0\" >0.2732</td>\n",
       "      <td id=\"T_def24_row3_col1\" class=\"data row3 col1\" >0.2732</td>\n",
       "      <td id=\"T_def24_row3_col2\" class=\"data row3 col2\" >0.5227</td>\n",
       "      <td id=\"T_def24_row3_col3\" class=\"data row3 col3\" >-0.1211</td>\n",
       "      <td id=\"T_def24_row3_col4\" class=\"data row3 col4\" >0.3623</td>\n",
       "      <td id=\"T_def24_row3_col5\" class=\"data row3 col5\" >0.3319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_def24_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_def24_row4_col0\" class=\"data row4 col0\" >0.2639</td>\n",
       "      <td id=\"T_def24_row4_col1\" class=\"data row4 col1\" >0.2639</td>\n",
       "      <td id=\"T_def24_row4_col2\" class=\"data row4 col2\" >0.5137</td>\n",
       "      <td id=\"T_def24_row4_col3\" class=\"data row4 col3\" >-0.0782</td>\n",
       "      <td id=\"T_def24_row4_col4\" class=\"data row4 col4\" >0.3561</td>\n",
       "      <td id=\"T_def24_row4_col5\" class=\"data row4 col5\" >0.3001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_def24_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_def24_row5_col0\" class=\"data row5 col0\" >0.2697</td>\n",
       "      <td id=\"T_def24_row5_col1\" class=\"data row5 col1\" >0.2697</td>\n",
       "      <td id=\"T_def24_row5_col2\" class=\"data row5 col2\" >0.5193</td>\n",
       "      <td id=\"T_def24_row5_col3\" class=\"data row5 col3\" >-0.1082</td>\n",
       "      <td id=\"T_def24_row5_col4\" class=\"data row5 col4\" >0.3600</td>\n",
       "      <td id=\"T_def24_row5_col5\" class=\"data row5 col5\" >0.3159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_def24_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_def24_row6_col0\" class=\"data row6 col0\" >0.2657</td>\n",
       "      <td id=\"T_def24_row6_col1\" class=\"data row6 col1\" >0.2657</td>\n",
       "      <td id=\"T_def24_row6_col2\" class=\"data row6 col2\" >0.5154</td>\n",
       "      <td id=\"T_def24_row6_col3\" class=\"data row6 col3\" >-0.0833</td>\n",
       "      <td id=\"T_def24_row6_col4\" class=\"data row6 col4\" >0.3573</td>\n",
       "      <td id=\"T_def24_row6_col5\" class=\"data row6 col5\" >0.3088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_def24_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_def24_row7_col0\" class=\"data row7 col0\" >0.2669</td>\n",
       "      <td id=\"T_def24_row7_col1\" class=\"data row7 col1\" >0.2669</td>\n",
       "      <td id=\"T_def24_row7_col2\" class=\"data row7 col2\" >0.5166</td>\n",
       "      <td id=\"T_def24_row7_col3\" class=\"data row7 col3\" >-0.0876</td>\n",
       "      <td id=\"T_def24_row7_col4\" class=\"data row7 col4\" >0.3581</td>\n",
       "      <td id=\"T_def24_row7_col5\" class=\"data row7 col5\" >0.3121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_def24_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_def24_row8_col0\" class=\"data row8 col0\" >0.2676</td>\n",
       "      <td id=\"T_def24_row8_col1\" class=\"data row8 col1\" >0.2676</td>\n",
       "      <td id=\"T_def24_row8_col2\" class=\"data row8 col2\" >0.5173</td>\n",
       "      <td id=\"T_def24_row8_col3\" class=\"data row8 col3\" >-0.1002</td>\n",
       "      <td id=\"T_def24_row8_col4\" class=\"data row8 col4\" >0.3586</td>\n",
       "      <td id=\"T_def24_row8_col5\" class=\"data row8 col5\" >0.3266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_def24_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_def24_row9_col0\" class=\"data row9 col0\" >0.2627</td>\n",
       "      <td id=\"T_def24_row9_col1\" class=\"data row9 col1\" >0.2627</td>\n",
       "      <td id=\"T_def24_row9_col2\" class=\"data row9 col2\" >0.5125</td>\n",
       "      <td id=\"T_def24_row9_col3\" class=\"data row9 col3\" >-0.0694</td>\n",
       "      <td id=\"T_def24_row9_col4\" class=\"data row9 col4\" >0.3553</td>\n",
       "      <td id=\"T_def24_row9_col5\" class=\"data row9 col5\" >0.3039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_def24_level0_row10\" class=\"row_heading level0 row10\" >Mean</th>\n",
       "      <td id=\"T_def24_row10_col0\" class=\"data row10 col0\" >0.2672</td>\n",
       "      <td id=\"T_def24_row10_col1\" class=\"data row10 col1\" >0.2672</td>\n",
       "      <td id=\"T_def24_row10_col2\" class=\"data row10 col2\" >0.5169</td>\n",
       "      <td id=\"T_def24_row10_col3\" class=\"data row10 col3\" >-0.0926</td>\n",
       "      <td id=\"T_def24_row10_col4\" class=\"data row10 col4\" >0.3583</td>\n",
       "      <td id=\"T_def24_row10_col5\" class=\"data row10 col5\" >0.3131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_def24_level0_row11\" class=\"row_heading level0 row11\" >Std</th>\n",
       "      <td id=\"T_def24_row11_col0\" class=\"data row11 col0\" >0.0042</td>\n",
       "      <td id=\"T_def24_row11_col1\" class=\"data row11 col1\" >0.0042</td>\n",
       "      <td id=\"T_def24_row11_col2\" class=\"data row11 col2\" >0.0040</td>\n",
       "      <td id=\"T_def24_row11_col3\" class=\"data row11 col3\" >0.0190</td>\n",
       "      <td id=\"T_def24_row11_col4\" class=\"data row11 col4\" >0.0028</td>\n",
       "      <td id=\"T_def24_row11_col5\" class=\"data row11 col5\" >0.0100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f8d8764e2e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation Pipeline and Model Successfully Saved\n"
     ]
    }
   ],
   "source": [
    "algos=['dt', 'catboost']\n",
    "loadconfig=False\n",
    "loadmodels=[False, False, False, False]\n",
    "\n",
    "for i in range(1):\n",
    "    dtmodels=[]\n",
    "    if loadconfig:\n",
    "        load_config(root+\"/configs/\" + \"x\" + str(i) + \"config.pkl\")\n",
    "    else:\n",
    "#        print(\"x\" + str(i) + \"dtcnt.csv\", end=\"\")\n",
    "#        traindf = pd.read_csv(\"x\" + str(i) + \"dtcnt.csv\")\n",
    "        print(\"xall.csv\", end=\"\")\n",
    "        traindf = pd.read_csv(\"xall.csv\")\n",
    "\n",
    "        for col in [col for col in traindf.columns if (col[:len(col)-6] in ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']) or ('yearmonth' in col) or ('day_of_week' in col)]:\n",
    "            traindf[col] = traindf[col].astype(str)\n",
    "            traindf[col] = traindf[col].astype('category')\n",
    "\n",
    "        s = setup(data=traindf, target='target',\n",
    "              transformation=True,\n",
    "              normalize=True,\n",
    "              ignore_features=['customer_ID'],\n",
    "              categorical_features = [col for col in traindf.columns if (col[:len(col)-6] in ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']) or ('yearmonth' in col) or ('day_of_week' in col)],\n",
    "              numeric_features=[col for col in traindf.columns if ('day_of_month') in col or ('day_of_year' in col)],\n",
    "#              remove_outliers=True, handle_unknown_categorical=True,\n",
    "#              combine_rare_levels=True, \n",
    "#              create_clusters=True,\n",
    "              transform_target=True, transform_target_method='yeo-johnson',\n",
    "              trigonometry_features=True,\n",
    "              polynomial_features=True, #polynomial_degree=4,\n",
    "              feature_selection=True,# remove_multicollinearity=True, pca=True, ignore_low_variance=True,\n",
    "              train_size=0.9, fold=10,\n",
    "              use_gpu=True,\n",
    "              silent = True, session_id = 123, verbose=True)\n",
    "        save_config(root+\"/configs/\" + \"x\" + str(i) + \"config.pkl\")\n",
    "\n",
    "    for j in range(len(algos)):\n",
    "        if loadmodels[j]:\n",
    "            dtmodels.append(load_model(root+\"/models/\" + algos[j] + \"-x\"+str(i)+\"reg\"))\n",
    "        else:\n",
    "            try:\n",
    "                dtmodels.append(create_model(algos[j], fold=10, verbose=True))\n",
    "                save_model(dtmodels[j], root+\"/models/\" + algos[j] + \"-x\"+str(i)+\"reg\")\n",
    "#                try:\n",
    "#                    dtmodels[j] = tune_model(dtmodels[j], fold=10, n_iter=15, verbose=True)\n",
    "#                    save_model(dtmodels[j], root+\"/models/\" + algos[j] + \"-x\"+str(i)+\"reg\")\n",
    "#                except:\n",
    "#                    print(\"Unable to tune for: \", algos[j])\n",
    "            except:\n",
    "                print(\"Unable to build model for:\", algos[j])\n",
    "\n",
    "\n",
    "    stacked=stack_models(dtmodels, meta_model=dtmodels[len(dtmodels)-1])\n",
    "    save_model(stacked, root+\"/models/\" + \"x\" + str(i) + \"stackedreg\")\n",
    "#    try:\n",
    "#        stacked = tune_model(stacked, fold=10, n_iter=15, verbose=True)\n",
    "#        save_model(stacked, root+\"/models/\" + \"x\" + str(i) + \"stackedreg\")\n",
    "#    except:\n",
    "#        print(\"Unable to tune for stacking\")\n",
    "    dtmodels.append(stacked)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ed6fc6-31c6-4c2b-8d0a-8bb490b679b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtmodels=[]\n",
    "dtsets=[]\n",
    "headers=pd.read_csv(root+\"/trains/headers.csv\")\n",
    "headers = headers.columns\n",
    "n=18\n",
    "algos=['dt', 'catboost', 'lightgbm']\n",
    "\n",
    "for i in range(1):\n",
    "#    for algo in algos:\n",
    "#        if algo == \"catboost\":\n",
    "#            dtmodels.append(load_model(root+\"/models/\" + algo + str(n)+\"-\"+str(i)+\"reg\"))\n",
    "#    dtmodels.append(load_model(root+\"/models/\" + \"\".join(algos) + str(n) + \"-\" + str(i) + \"stackedreg\"))\n",
    "    dtmodels.append(load_model(root+\"/models/\" + \"x\" + str(i) + \"stackedreg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5015f2fa-3ea3-4824-93d3-b038cb348094",
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked=stack_models(dtmodels, meta_model=dtmodels[0])\n",
    "#stacked = tune_model(stacked, fold=10, n_iter=15, verbose=True)\n",
    "dtmodels.append(stacked)\n",
    "save_model(stacked, root+\"/models/\" + \"x\" + str(i) + \"stackedreg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f70d20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152, '/home/sadagopan/amex-default-prediction/test/testfv')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tests = []\n",
    "for r, d, f in os.walk(root+\"test\"):\n",
    "#    print(r)\n",
    "#    if r ==root+'/test':\n",
    "        f.sort()\n",
    "        for file in f:\n",
    "            if '.idx' not in file and '.csv' not in file and 'cust' not in file and 'preds' not in file and ('testa' in file or 'testb' in file or 'testc' in file or 'testd' in file or 'teste' in file or 'testf' in file):\n",
    "                tests.append(os.path.join(r, file))\n",
    "len(tests), tests[len(tests)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f23246-cd6b-43cc-92bd-6ede1e4ce46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#traindf = pd.concat(traindfs, ignore_index=True)\n",
    "headers=pd.read_csv(root+\"headers.csv\")\n",
    "headers = headers.columns\n",
    "\n",
    "testdf = pd.read_csv(root+\"/tstdata\")\n",
    "tdf = testdf.groupby('customer_ID')['S_2'].count().reset_index()\n",
    "for i in range(1, 13):\n",
    "    print(i, tdf[tdf['S_2']==i].count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "99d865ad-7821-4ebc-9777-2d2cb228e393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer_ID    811329\n",
      "S_2            811329\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(tdf[tdf['S_2']==2].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a973a321-f6df-4e18-b7f6-ea3b7aeaa89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "testdfs = []\n",
    "\n",
    "for k in range(12):\n",
    "    testdfs.append(pd.DataFrame(columns=headers))\n",
    "\n",
    "for i in range(len(tests)):\n",
    "    print(tests[i])\n",
    "    testdf = pd.read_csv(tests[i], names=headers, header=None)\n",
    "    for j in range(1, 13):\n",
    "        mask = testdf.customer_ID.isin(tdf[tdf['S_2']==j].customer_ID)\n",
    "        testdfs[j-1] = pd.concat([testdfs[j-1], testdf[mask]])\n",
    "\n",
    "for i in range(len(testdfs)):\n",
    "    testdfs[i].to_csv(\"t\" + str(i) + \".csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e29f64d-7eeb-45e8-ab56-c30d7ccdb502",
   "metadata": {},
   "outputs": [],
   "source": [
    "testdfs = []\n",
    "\n",
    "for k in range(13):\n",
    "    testdfs.append(pd.DataFrame(columns=headers))\n",
    "k=0\n",
    "for i in range(len(tests)):\n",
    "    print(tests[i])\n",
    "    testdf = pd.read_csv(tests[i], names=headers, header=None)\n",
    "    for j in range(13, 14):\n",
    "        mask = testdf.customer_ID.isin(tdf[tdf['S_2']==j].customer_ID)\n",
    "        testdfs[j-1] = pd.concat([testdfs[j-1], testdf[mask]], ignore_index=True)\n",
    "        if testdfs[j-1]['customer_ID'].count() >= 260000:\n",
    "            testdfs[j-1].iloc[:260000,].to_csv(\"t\" + str(j-1) + \"-\" + str(k) + \".csv\", index=False)\n",
    "            testdfs[j-1] = testdfs[j-1].iloc[260000:,]\n",
    "            k+=1            \n",
    "\n",
    "testdfs[12].to_csv(\"t12-\" + str(k) + \".csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98dc7126-8f31-4205-9396-e79b00191e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t11dtcnt.csv\n"
     ]
    }
   ],
   "source": [
    "predictions=pd.DataFrame()\n",
    "for i in range(11, 12): #, 12):\n",
    "    model = dtmodels[len(dtmodels)-1] #load_model(root+\"/models/\" + \"x\" + str(i) + \"stackedreg\")\n",
    "    print(\"t\" + str(i) + \"dtcnt.csv\")\n",
    "    testdf = (pd.read_csv(\"t\" + str(i) + \"dtcnt.csv\"))\n",
    "\n",
    "    preds = predict_model(model, data=testdf, verbose=True)\n",
    "    predictions = pd.concat([predictions, preds], ignore_index=True, axis=0)\n",
    "    preds[['customer_ID', 'Label']].to_csv(root+\"/test/t\"+ str(i) + \".dtcnt\" + \".preds\", header=False, index=False)\n",
    "\n",
    "#predictions.to_csv(\"test.preds\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "57716a7c-4d84-4742-88ea-0e498b7fe382",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t0.csv\n",
      "t1.csv\n",
      "t2.csv\n",
      "t3.csv\n",
      "t4.csv\n",
      "t5.csv\n",
      "t6.csv\n",
      "t7.csv\n",
      "t8.csv\n",
      "t9.csv\n",
      "t10.csv\n",
      "t11.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_ID</th>\n",
       "      <th>B_1201804</th>\n",
       "      <th>B_10201804</th>\n",
       "      <th>B_11201804</th>\n",
       "      <th>B_12201804</th>\n",
       "      <th>B_13201804</th>\n",
       "      <th>B_14201804</th>\n",
       "      <th>B_15201804</th>\n",
       "      <th>B_16201804</th>\n",
       "      <th>B_17201804</th>\n",
       "      <th>...</th>\n",
       "      <th>day_of_week201804</th>\n",
       "      <th>day_of_year201804</th>\n",
       "      <th>dncnt201804</th>\n",
       "      <th>ncount201804</th>\n",
       "      <th>pncnt201804</th>\n",
       "      <th>rncnt201804</th>\n",
       "      <th>sncnt201804</th>\n",
       "      <th>week_of_year201804</th>\n",
       "      <th>yearmonth201804</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0004ef34876259ea444bbe3c15031240a66cc73a7db0ef...</td>\n",
       "      <td>0.001889</td>\n",
       "      <td>0.297802</td>\n",
       "      <td>0.004482</td>\n",
       "      <td>0.415754</td>\n",
       "      <td>0.383141</td>\n",
       "      <td>0.012555</td>\n",
       "      <td>0.007515</td>\n",
       "      <td>0.085317</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>114</td>\n",
       "      <td>1062</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>201904</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00085c285868ee0995a0d47eb096a229720abbf0fc8e3a...</td>\n",
       "      <td>0.005638</td>\n",
       "      <td>0.153685</td>\n",
       "      <td>0.007183</td>\n",
       "      <td>0.009568</td>\n",
       "      <td>0.007382</td>\n",
       "      <td>0.007703</td>\n",
       "      <td>0.000538</td>\n",
       "      <td>0.007939</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>1147</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>36</td>\n",
       "      <td>15</td>\n",
       "      <td>201904</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002ea4ea3c03186c1d6a1c375843c85f17ff54a994f3e2...</td>\n",
       "      <td>0.158615</td>\n",
       "      <td>-0.000709</td>\n",
       "      <td>0.116474</td>\n",
       "      <td>0.013808</td>\n",
       "      <td>0.008102</td>\n",
       "      <td>0.035846</td>\n",
       "      <td>0.009827</td>\n",
       "      <td>0.004714</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>107</td>\n",
       "      <td>3083</td>\n",
       "      <td>61</td>\n",
       "      <td>3</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>201904</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>004900af6bce781c1538401a899a8a7ccccab64b95896b...</td>\n",
       "      <td>0.222732</td>\n",
       "      <td>0.298115</td>\n",
       "      <td>0.162289</td>\n",
       "      <td>0.005087</td>\n",
       "      <td>0.005549</td>\n",
       "      <td>0.028454</td>\n",
       "      <td>0.000634</td>\n",
       "      <td>0.006255</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>117</td>\n",
       "      <td>3168</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>201904</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00639c0178fd720eb3e565558120a8b6bf040451cb7a88...</td>\n",
       "      <td>0.059552</td>\n",
       "      <td>0.133349</td>\n",
       "      <td>0.056337</td>\n",
       "      <td>0.112435</td>\n",
       "      <td>0.094728</td>\n",
       "      <td>0.128300</td>\n",
       "      <td>0.005942</td>\n",
       "      <td>0.591555</td>\n",
       "      <td>0.008546</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>114</td>\n",
       "      <td>1057</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>201904</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3899</th>\n",
       "      <td>ffb1ca6570e67b37ba9f0817d0686200e2d527c4590ef6...</td>\n",
       "      <td>0.061558</td>\n",
       "      <td>0.028597</td>\n",
       "      <td>0.040369</td>\n",
       "      <td>0.008627</td>\n",
       "      <td>0.009395</td>\n",
       "      <td>0.019425</td>\n",
       "      <td>0.009200</td>\n",
       "      <td>0.005948</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>105</td>\n",
       "      <td>2117</td>\n",
       "      <td>47</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>201904</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3900</th>\n",
       "      <td>ffc3e0d0c0681e021cd305e583b69c4783a2d162faf18a...</td>\n",
       "      <td>0.192003</td>\n",
       "      <td>0.005672</td>\n",
       "      <td>0.152140</td>\n",
       "      <td>0.011209</td>\n",
       "      <td>0.012303</td>\n",
       "      <td>0.044810</td>\n",
       "      <td>0.002728</td>\n",
       "      <td>0.007848</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>109</td>\n",
       "      <td>3160</td>\n",
       "      <td>63</td>\n",
       "      <td>3</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>201904</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3901</th>\n",
       "      <td>ffd14c0fe5226c6253141aa41a62a502a9ff8a9970b4e5...</td>\n",
       "      <td>0.009424</td>\n",
       "      <td>0.005620</td>\n",
       "      <td>0.004027</td>\n",
       "      <td>0.093704</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.025862</td>\n",
       "      <td>0.027792</td>\n",
       "      <td>0.008456</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>114</td>\n",
       "      <td>3089</td>\n",
       "      <td>62</td>\n",
       "      <td>3</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>201904</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3902</th>\n",
       "      <td>ffd9110c2b6cc0afa50e3adb31e38386379419bacfd649...</td>\n",
       "      <td>0.180783</td>\n",
       "      <td>0.061928</td>\n",
       "      <td>0.142025</td>\n",
       "      <td>0.011307</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.046140</td>\n",
       "      <td>0.005505</td>\n",
       "      <td>0.009100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>115</td>\n",
       "      <td>3168</td>\n",
       "      <td>65</td>\n",
       "      <td>3</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>201904</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3903</th>\n",
       "      <td>fff7235d38f0e0d547db83499e7e0392c54f94ea649397...</td>\n",
       "      <td>0.008536</td>\n",
       "      <td>0.003042</td>\n",
       "      <td>0.006478</td>\n",
       "      <td>0.009279</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008103</td>\n",
       "      <td>0.002247</td>\n",
       "      <td>0.002483</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>092</td>\n",
       "      <td>2224</td>\n",
       "      <td>53</td>\n",
       "      <td>3</td>\n",
       "      <td>37</td>\n",
       "      <td>27</td>\n",
       "      <td>14</td>\n",
       "      <td>201904</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3904 rows  201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            customer_ID  B_1201804  \\\n",
       "0     0004ef34876259ea444bbe3c15031240a66cc73a7db0ef...   0.001889   \n",
       "1     00085c285868ee0995a0d47eb096a229720abbf0fc8e3a...   0.005638   \n",
       "2     002ea4ea3c03186c1d6a1c375843c85f17ff54a994f3e2...   0.158615   \n",
       "3     004900af6bce781c1538401a899a8a7ccccab64b95896b...   0.222732   \n",
       "4     00639c0178fd720eb3e565558120a8b6bf040451cb7a88...   0.059552   \n",
       "...                                                 ...        ...   \n",
       "3899  ffb1ca6570e67b37ba9f0817d0686200e2d527c4590ef6...   0.061558   \n",
       "3900  ffc3e0d0c0681e021cd305e583b69c4783a2d162faf18a...   0.192003   \n",
       "3901  ffd14c0fe5226c6253141aa41a62a502a9ff8a9970b4e5...   0.009424   \n",
       "3902  ffd9110c2b6cc0afa50e3adb31e38386379419bacfd649...   0.180783   \n",
       "3903  fff7235d38f0e0d547db83499e7e0392c54f94ea649397...   0.008536   \n",
       "\n",
       "      B_10201804  B_11201804  B_12201804  B_13201804  B_14201804  B_15201804  \\\n",
       "0       0.297802    0.004482    0.415754    0.383141    0.012555    0.007515   \n",
       "1       0.153685    0.007183    0.009568    0.007382    0.007703    0.000538   \n",
       "2      -0.000709    0.116474    0.013808    0.008102    0.035846    0.009827   \n",
       "3       0.298115    0.162289    0.005087    0.005549    0.028454    0.000634   \n",
       "4       0.133349    0.056337    0.112435    0.094728    0.128300    0.005942   \n",
       "...          ...         ...         ...         ...         ...         ...   \n",
       "3899    0.028597    0.040369    0.008627    0.009395    0.019425    0.009200   \n",
       "3900    0.005672    0.152140    0.011209    0.012303    0.044810    0.002728   \n",
       "3901    0.005620    0.004027    0.093704         NaN    0.025862    0.027792   \n",
       "3902    0.061928    0.142025    0.011307         NaN    0.046140    0.005505   \n",
       "3903    0.003042    0.006478    0.009279         NaN    0.008103    0.002247   \n",
       "\n",
       "      B_16201804  B_17201804  ...  day_of_week201804  day_of_year201804  \\\n",
       "0       0.085317         NaN  ...                  2                114   \n",
       "1       0.007939         NaN  ...                  2                100   \n",
       "2       0.004714         NaN  ...                  2                107   \n",
       "3       0.006255         NaN  ...                  5                117   \n",
       "4       0.591555    0.008546  ...                  2                114   \n",
       "...          ...         ...  ...                ...                ...   \n",
       "3899    0.005948         NaN  ...                  7                105   \n",
       "3900    0.007848         NaN  ...                  4                109   \n",
       "3901    0.008456         NaN  ...                  2                114   \n",
       "3902    0.009100         NaN  ...                  3                115   \n",
       "3903    0.002483         NaN  ...                  1                092   \n",
       "\n",
       "      dncnt201804  ncount201804  pncnt201804  rncnt201804  sncnt201804  \\\n",
       "0            1062            24            0           37            0   \n",
       "1            1147            30            0           37           36   \n",
       "2            3083            61            3           61            0   \n",
       "3            3168            64            3           61            0   \n",
       "4            1057            27            0           37            0   \n",
       "...           ...           ...          ...          ...          ...   \n",
       "3899         2117            47            3           27            0   \n",
       "3900         3160            63            3           61            0   \n",
       "3901         3089            62            3           61            0   \n",
       "3902         3168            65            3           61            0   \n",
       "3903         2224            53            3           37           27   \n",
       "\n",
       "      week_of_year201804  yearmonth201804  count  \n",
       "0                     17           201904      1  \n",
       "1                     15           201904      1  \n",
       "2                     16           201904      1  \n",
       "3                     17           201904      1  \n",
       "4                     17           201904      1  \n",
       "...                  ...              ...    ...  \n",
       "3899                  15           201904      1  \n",
       "3900                  16           201904      1  \n",
       "3901                  17           201904      1  \n",
       "3902                  17           201904      1  \n",
       "3903                  14           201904      1  \n",
       "\n",
       "[3904 rows x 201 columns]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdfs = []\n",
    "i=0\n",
    "for j in range(12):\n",
    "    print(\"t\" + str(j) + \".csv\")\n",
    "    testdfs.append(pd.read_csv(\"t\" + str(j) + \".csv\"))\n",
    "\n",
    "    mindate = datetime.strptime('Apr 1 2018  1:33PM', '%b %d %Y %I:%M%p')\n",
    "    maxdate = mindate + pd.offsets.DateOffset(months=j)\n",
    "    dates=pd.DataFrame(pd.date_range(mindate, maxdate, freq='MS').tolist(), columns=['S_2_dt'])\n",
    "    dates['yearmonth'] = dates['S_2_dt'].dt.strftime(\"%Y%m\")\n",
    "\n",
    "    testdfs[i]['S_2'] = pd.to_datetime(testdfs[i]['S_2'])\n",
    "    testdfs[i]['yearmonth'] = testdfs[i]['S_2'].dt.strftime(\"%Y%m\")\n",
    "    testdfs[i][['sncnt', 'dncnt', 'rncnt', 'bncnt', 'pncnt']] = testdfs[i].apply(lambda x: (sum([(2^scols.index(col)) for col in scols if pd.isnull(x[col])]),\n",
    "                sum([(2^dcols.index(col)) for col in dcols if pd.isnull(x[col])]),\n",
    "                sum([(2^rcols.index(col)) for col in rcols if pd.isnull(x[col])]),\n",
    "                sum([(2^bcols.index(col)) for col in bcols if pd.isnull(x[col])]),\n",
    "                sum([(2^pcols.index(col)) for col in pcols if pd.isnull(x[col])])), axis=1, result_type='expand')\n",
    "\n",
    "    dateshift = pd.DataFrame(testdfs[i].groupby(['customer_ID'])['S_2'].agg(['min', 'count'])).rename(columns={\"min\": \"S_2\"}).reset_index()\n",
    "    dateshift['S_2'] = pd.to_datetime(dateshift['S_2'])\n",
    "    dateshift['mondiff'] = dateshift['S_2'].dt.month - 4\n",
    "    dateshift['mondiff'] = dateshift['mondiff'].apply(lambda x: 12+x if x <=0 else x)\n",
    "\n",
    "    testdfs[i] = pd.merge(testdfs[i], dateshift[['customer_ID', 'mondiff']], on=['customer_ID'], how='inner')\n",
    "    testdfs[i]['S_2'] = testdfs[i].apply(lambda x: x['S_2'] - pd.offsets.DateOffset(months=x['mondiff']), axis=1)\n",
    "    testdfs[i]['S_2'] = testdfs[i].apply(lambda x: x['S_2'] - pd.offsets.DateOffset(years=1) if x['S_2'].year==2019 and x['S_2'].month > 4 else x['S_2'], axis=1)\n",
    "    testdfs[i]['yearmonthnew'] = testdfs[i]['S_2'].dt.strftime(\"%Y%m\")\n",
    "    testdfs[i]['ncount'] = testdfs[i].isnull().sum(axis=1)\n",
    "\n",
    "    testdfs[i]=pd.merge(testdfs[i], pd.merge(pd.DataFrame(testdfs[i]['customer_ID'].unique(), columns=['customer_ID']), dates, how='cross'), left_on=['customer_ID', 'yearmonthnew'], right_on=['customer_ID', 'yearmonth'], how='inner')\n",
    "    testdfs[i]['S_2'] = testdfs[i].apply(lambda x: x['S_2_dt'] if pd.isnull(x['S_2']) else x['S_2'], axis=1)\n",
    "    testdfs[i].drop(columns=['S_2_dt', 'yearmonth_y'], inplace=True)\n",
    "    testdfs[i] = testdfs[i].rename(columns={\"yearmonth_x\": \"yearmonth\"})\n",
    "\n",
    "    testdfs[i]['mondiff'].fillna(0, inplace=True)\n",
    "    testdfs[i]['day_of_month'] = testdfs[i]['S_2'].dt.strftime(\"%d\")\n",
    "    testdfs[i]['day_of_week'] = testdfs[i]['S_2'].dt.strftime(\"%u\")\n",
    "    testdfs[i]['day_of_year'] = testdfs[i]['S_2'].dt.strftime(\"%j\")\n",
    "    testdfs[i]['week_of_year'] = testdfs[i]['S_2'].dt.isocalendar().week\n",
    "    testdfs[i].drop(columns=['mondiff'], inplace=True)\n",
    "    testdfs[i].drop(columns=['S_2'], inplace=True)\n",
    "\n",
    "#    df = testdfs[i].groupby(\"day_of_year\").transform(lambda x: x.fillna(x.mean()))\n",
    "#    for col in testdfs[i][testdfs[i].select_dtypes(include=['int64', 'float64']).columns].isna().any().index:\n",
    "#        testdfs[i][col] = df[col]\n",
    "    \n",
    "#    df = testdfs[i].groupby(\"week_of_year\").transform(lambda x: x.fillna(x.mean()))\n",
    "#    for col in testdfs[i][testdfs[i].select_dtypes(include=['int64', 'float64']).columns].isna().any().index:\n",
    "#        testdfs[i][col] = df[col]\n",
    "\n",
    "#    df = testdfs[i].groupby(\"day_of_month\").transform(lambda x: x.fillna(x.mean()))\n",
    "#    for col in testdfs[i][testdfs[i].select_dtypes(include=['int64', 'float64']).columns].isna().any().index:\n",
    "#        testdfs[i][col] = df[col]\n",
    "\n",
    "#    df = testdfs[i].groupby(\"yearmonth\").transform(lambda x: x.fillna(x.mean()))\n",
    "#    for col in testdfs[i][testdfs[i].select_dtypes(include=['int64', 'float64']).columns].isna().any().index:\n",
    "#        testdfs[i][col] = df[col]\n",
    "\n",
    "#    df = testdfs[i].groupby(\"day_of_week\").transform(lambda x: x.fillna(x.mean()))\n",
    "#    for col in testdfs[i][testdfs[i].select_dtypes(include=['int64', 'float64']).columns].isna().any().index:\n",
    "#        testdfs[i][col] = df[col]\n",
    "\n",
    "    testdfs[i]=testdfs[i].set_index(['customer_ID', 'yearmonthnew']).unstack(1).sort_index(axis=1, level=1)\n",
    "    testdfs[i] = testdfs[i].reset_index()\n",
    "    testdfs[i] = pd.merge(testdfs[i], dateshift[['customer_ID', 'count']], on=['customer_ID'], how='inner')\n",
    "\n",
    "#    for col in testdfs[i][testdfs[i].select_dtypes(include=['int64', 'float64']).columns].isna().any().index:\n",
    "#        testdfs[i][col].fillna(testdfs[i][[column for column in testdfs[i].columns.tolist() if col[:len(col)-3] == column[:len(column)-3]]].mean(axis=1), inplace=True)\n",
    "\n",
    "#    for col in testdfs[i][testdfs[i].select_dtypes(include=['int64', 'float64']).columns].isna().any().index:\n",
    "#        testdfs[i][col].fillna(testdfs[i][col].mean(), inplace=True)\n",
    "\n",
    "    testdfs[i].drop(columns=[testdfs[i].columns.to_list()[1]], inplace=True)\n",
    "\n",
    "    coldict={}\n",
    "    for col in testdfs[i].columns:\n",
    "        coldict[col] = ''.join(col)\n",
    "    testdfs[i].rename(coldict, axis=1, inplace=True)\n",
    "\n",
    "    testdfs[i].to_csv(\"t\" + str(j) +\"dtcnt.csv\", index=False)\n",
    "    i+=1\n",
    "\n",
    "testdfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51b9fd3-3869-491a-8ee4-da420f3955d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "testdfs = []\n",
    "i=0\n",
    "for j in range(41):\n",
    "    print(\"t12-\" + str(j) + \".csv\")\n",
    "    testdfs.append(pd.read_csv(\"t12-\" + str(j) + \".csv\"))\n",
    "\n",
    "    mindate = datetime.strptime('Apr 1 2018  1:33PM', '%b %d %Y %I:%M%p')\n",
    "    maxdate = mindate + pd.offsets.DateOffset(months=12)\n",
    "    dates=pd.DataFrame(pd.date_range(mindate, maxdate, freq='MS').tolist(), columns=['S_2_dt'])\n",
    "    dates['yearmonth'] = dates['S_2_dt'].dt.strftime(\"%Y%m\")\n",
    "\n",
    "    testdfs[i]['S_2'] = pd.to_datetime(testdfs[i]['S_2'])\n",
    "    testdfs[i]['yearmonth'] = testdfs[i]['S_2'].dt.strftime(\"%Y%m\")\n",
    "\n",
    "    dateshift = pd.DataFrame(testdfs[i].groupby(['customer_ID'])['S_2'].agg(['min', 'count'])).rename(columns={\"min\": \"S_2\"}).reset_index()\n",
    "    dateshift['S_2'] = pd.to_datetime(dateshift['S_2'])\n",
    "    dateshift['mondiff'] = dateshift['S_2'].dt.month - 4\n",
    "    dateshift['mondiff'] = dateshift['mondiff'].apply(lambda x: 12+x if x <0 else x)\n",
    "\n",
    "    testdfs[i] = pd.merge(testdfs[i], dateshift[['customer_ID', 'mondiff']], on=['customer_ID'], how='inner')\n",
    "    testdfs[i]['S_2'] = testdfs[i].apply(lambda x: x['S_2'] - pd.offsets.DateOffset(months=x['mondiff']), axis=1)\n",
    "    testdfs[i]['S_2'] = testdfs[i].apply(lambda x: x['S_2'] - pd.offsets.DateOffset(years=1) if x['S_2'].year==2019 and x['S_2'].month > 4 else x['S_2'], axis=1)\n",
    "    testdfs[i]['yearmonthnew'] = testdfs[i]['S_2'].dt.strftime(\"%Y%m\")\n",
    "#    testdfs[i]['ncount'] = testdfs[i].isnull().sum(axis=1)\n",
    "\n",
    "    testdfs[i]=pd.merge(testdfs[i], pd.merge(pd.DataFrame(testdfs[i]['customer_ID'].unique(), columns=['customer_ID']), dates, how='cross'), left_on=['customer_ID', 'yearmonthnew'], right_on=['customer_ID', 'yearmonth'], how='inner')\n",
    "    testdfs[i]['S_2'] = testdfs[i].apply(lambda x: x['S_2_dt'] if pd.isnull(x['S_2']) else x['S_2'], axis=1)\n",
    "    testdfs[i].drop(columns=['S_2_dt', 'yearmonth_y'], inplace=True)\n",
    "    testdfs[i] = testdfs[i].rename(columns={\"yearmonth_x\": \"yearmonth\"})\n",
    "\n",
    "    testdfs[i]['mondiff'].fillna(0, inplace=True)\n",
    "    testdfs[i]['day_of_month'] = testdfs[i]['S_2'].dt.strftime(\"%d\")\n",
    "    testdfs[i]['day_of_week'] = testdfs[i]['S_2'].dt.strftime(\"%u\")\n",
    "    testdfs[i]['day_of_year'] = testdfs[i]['S_2'].dt.strftime(\"%j\")\n",
    "    testdfs[i].drop(columns=['mondiff'], inplace=True)\n",
    "    testdfs[i].drop(columns=['S_2'], inplace=True)\n",
    "\n",
    "    testdfs[i]=testdfs[i].set_index(['customer_ID', 'yearmonthnew']).unstack(1).sort_index(axis=1, level=1)\n",
    "    testdfs[i] = testdfs[i].reset_index()\n",
    "    testdfs[i] = pd.merge(testdfs[i], dateshift[['customer_ID', 'count']], on=['customer_ID'], how='inner')\n",
    "\n",
    "    for col in testdfs[i][testdfs[i].select_dtypes(include=['int64', 'float64']).columns].isna().any().index:\n",
    "        testdfs[i][col].fillna(testdfs[i][[column for column in testdfs[i].columns.tolist() if col[:len(col)-3] == column[:len(column)-3]]].mean(axis=1), inplace=True)\n",
    "\n",
    "    for col in testdfs[i][testdfs[i].select_dtypes(include=['int64', 'float64']).columns].isna().any().index:\n",
    "        testdfs[i][col].fillna(testdfs[i][col].mean(), inplace=True)\n",
    "\n",
    "    testdfs[i].drop(columns=[testdfs[i].columns.to_list()[1]], inplace=True)\n",
    "\n",
    "    coldict={}\n",
    "    for col in testdfs[i].columns:\n",
    "        coldict[col] = ''.join(col)\n",
    "    testdfs[i].rename(coldict, axis=1, inplace=True)\n",
    "\n",
    "    testdfs[i].to_csv(\"t12-\" + str(j) +\"dtcnt.csv\", index=False)\n",
    "#    os.remove(tests[i]\n",
    "    i+=1\n",
    "\n",
    "testdfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5f9a41",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model = load_model(\"/amex/models/dt1Lclass\")\n",
    "#model = calibrated_dt\n",
    "#model=stacked\n",
    "#model=load_model(\"/amex/models/dtstackedreg\")\n",
    "\n",
    "useFlag = False\n",
    "mindate = datetime.strptime('Apr 1 2018  1:33PM', '%b %d %Y %I:%M%p')\n",
    "\n",
    "predictions=pd.DataFrame()\n",
    "for i in range(104, len(tests)):\n",
    "    print(tests[i].split(\"/\")[5], end =\" \")\n",
    "    if (not useFlag):\n",
    "        headers=pd.read_csv(root+\"headers.csv\")\n",
    "        headers = headers.columns\n",
    "        test = pd.read_csv(tests[i], names=headers, header=None)\n",
    "        test['S_2'] = pd.to_datetime(test['S_2'])\n",
    "\n",
    "        dateshift = pd.DataFrame(test.groupby(['customer_ID'])['S_2'].agg(['min', 'count'])).rename(columns={\"min\": \"S_2\"}).reset_index()\n",
    "        dateshift['S_2'] = pd.to_datetime(dateshift['S_2'])\n",
    "        dateshift['mondiff'] = ((dateshift['S_2'].dt.year * 12) + dateshift['S_2'].dt.month) - ((mindate.year*12) + mindate.month)\n",
    "\n",
    "        test = pd.merge(test, dateshift[['customer_ID', 'mondiff']], on=['customer_ID'], how='inner')\n",
    "        test['S_2'] = test.apply(lambda x: x['S_2'] - pd.offsets.DateOffset(months=x['mondiff']), axis=1)\n",
    "        test['yearmonth'] = test['S_2'].dt.strftime(\"%Y%m\")\n",
    "\n",
    "        test=pd.merge(test, pd.merge(pd.DataFrame(test['customer_ID'].unique(), columns=['customer_ID']), dates, how='cross'), on=['customer_ID', 'yearmonth'], how='outer')\n",
    "        test['S_2'] = test.apply(lambda x: x['S_2_dt'] if pd.isnull(x['S_2']) else x['S_2'], axis=1)\n",
    "\n",
    "        test['day_of_month'] = test['S_2'].dt.strftime(\"%d\")\n",
    "        test['day_of_week'] = test['S_2'].dt.strftime(\"%u\")\n",
    "        test['day_of_year'] = test['S_2'].dt.strftime(\"%j\")\n",
    "        test.drop(columns=['S_2_dt'], inplace=True)\n",
    "        test.drop(columns=['mondiff'], inplace=True)\n",
    "\n",
    "        test = test.set_index(['customer_ID', 'yearmonth']).unstack(1).sort_index(axis=1, level=1)\n",
    "        test = test.reset_index()\n",
    "        test = pd.merge(test, dateshift[['customer_ID', 'count']], on=['customer_ID'], how='inner')\n",
    "\n",
    "        test = test.merge(labels, on=\"customer_ID\", how=\"left\")\n",
    "        test.drop(columns=[test.columns.to_list()[1]], inplace=True)\n",
    "\n",
    "        coldict={}\n",
    "        for col in test.columns:\n",
    "            coldict[col] = ''.join(col)\n",
    "\n",
    "        test.rename(coldict, axis=1, inplace=True)\n",
    "        test['target'] = 0\n",
    "\n",
    "#        test.to_csv(tests[i]+\".csv\", index=False, header=False)\n",
    "    else:\n",
    "        headers=pd.read_csv(root+\"/trains/headers.csv\")\n",
    "        headers = headers.columns\n",
    "        test = pd.read_csv(tests[i]+\".csv\", names=headers, header=None)\n",
    "\n",
    "    for col in [col for col in test.columns if (col[:len(col)-6] in ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']) or ('yearmonth' in col) or ('day_of_week' in col)]:\n",
    "        test[col] = test[col].astype(str)\n",
    "        test[col] = test[col].astype('category')\n",
    "\n",
    "    test['customer_ID2'] = test['customer_ID']\n",
    "    test.drop(columns=['customer_ID'], inplace=True)\n",
    "    test.rename(columns = {\"customer_ID2\": \"customer_ID\"}, inplace=True)\n",
    "    try:\n",
    "        preds = predict_model(dtmodels[len(dtmodels)-1], data=test, verbose=True)\n",
    "    except:\n",
    "        preds = predict_model(dtmodels[0], data=test, verbose=True)\n",
    "    predictions = pd.concat([predictions, preds], ignore_index=True, axis=0)\n",
    "#    preds[['customer_ID', 'Label']].to_csv(tests[i]+\".\" + \"\".join(algos) + s\n",
    "    preds[['customer_ID', 'Label']].to_csv(tests[i]+\".dt2\" + \".preds\", header=False, index=False)\n",
    "\n",
    "predictions.to_csv(\"test.preds\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d33efa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "mindate = datetime.strptime('Apr 1 2018  1:33PM', '%b %d %Y %I:%M%p')\n",
    "\n",
    "test = pd.read_csv(tests[i], names=headers, header=None)\n",
    "for col in [col for col in test.columns if col[:len(col)-6] in ['B_30', 'B_38', 'D_114', 'D_116', 'D_117', 'D_120', 'D_126', 'D_63', 'D_64', 'D_66', 'D_68']]:\n",
    "    test[col] = test[col].astype(str)\n",
    "    test[col] = test[col].astype('category')\n",
    "\n",
    "test['S_2'] = pd.to_datetime(test['S_2'])\n",
    "\n",
    "dateshift = pd.DataFrame(test.groupby(['customer_ID'])['S_2'].min()).reset_index()\n",
    "dateshift['S_2'] = pd.to_datetime(dateshift['S_2'])\n",
    "dateshift['mondiff'] = ((dateshift['S_2'].dt.year * 12) + dateshift['S_2'].dt.month) - ((mindate.year*12) + mindate.month)\n",
    "\n",
    "test = pd.merge(test, dateshift[['customer_ID', 'mondiff']], on=['customer_ID'], how='inner')\n",
    "test['S_2'] = test.apply(lambda x: x['S_2'] - pd.offsets.DateOffset(months=x['mondiff']), axis=1)\n",
    "test['yearmonth'] = test['S_2'].dt.strftime(\"%Y%m\")\n",
    "\n",
    "test=pd.merge(test, pd.merge(pd.DataFrame(test['customer_ID'].unique(), columns=['customer_ID']), dates, how='cross'), on=['customer_ID', 'yearmonth'], how='outer')\n",
    "test['S_2'] = test.apply(lambda x: x['S_2_dt'] if pd.isnull(x['S_2']) else x['S_2'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dafe5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print((test[test['customer_ID']=='0a31ea77ca60c94e754c884616778043f32f445ad2765f9564567030b425c185']['S_2'][0].year*12 + test[test['customer_ID']=='0a31ea77ca60c94e754c884616778043f32f445ad2765f9564567030b425c185']['S_2'][0].month) - (mindate.year*12+mindate.month), (test[test['customer_ID']=='0a31ea77ca60c94e754c884616778043f32f445ad2765f9564567030b425c185']['S_2'][0].year*100 + test[test['customer_ID']=='0a31ea77ca60c94e754c884616778043f32f445ad2765f9564567030b425c185']['S_2'][0].month), (mindate.year*100+mindate.month))\n",
    "test\n",
    "test[test['customer_ID']=='0a31ea77ca60c94e754c884616778043f32f445ad2765f9564567030b425c185']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170965cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def testcolfillna(col):\n",
    "#    print(col, end=\"\")\n",
    "    try:\n",
    "        test[col] = test[col[:len(col)-6]+str(int(col[len(col)-6:len(col)-2])+1) + str(col[len(col)-2:len(col)])]\n",
    "    except:\n",
    "        print(col)\n",
    "    return\n",
    "\n",
    "def testfillna(col):\n",
    "#    print(\".\", end=\"\")\n",
    "    test[col].fillna(test[[column for column in test.columns.tolist() if column[:len(column)-3] in col]].mean(axis=1), inplace=True)\n",
    "\n",
    "#    with Pool(12) as pool:\n",
    "#        pool.map(testcolfillna, [col for col in test.columns[test.isna().all()]])\n",
    "\n",
    "#    with Pool(12) as pool:\n",
    "#        pool.map(testfillna, [col for col in test.columns[test.isna().any()]])\n",
    "\n",
    "#    for col in test.columns[test.isna().all()]:\n",
    "#        test[col] = test[col[:len(col)-6]+str(int(col[len(col)-6:len(col)-2])+1) + str(col[len(col)-2:len(col)])]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0ced3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf = pd.read_csv(root+\"/x12-0.csv\")\n",
    "traindf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8908c0de-5489-4c22-8e17-fddad200a3a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
